{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amritagupta/.conda/envs/streamflow/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# import copy\n",
    "# from argparse import Namespace\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(sys.path[0], os.pardir))\n",
    "sys.path.append(PROJECT_ROOT)\n",
    "from src.utils import parse_configargparse_args"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarize Ranking Model Training Experiment Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 324.07it/s]\n"
     ]
    }
   ],
   "source": [
    "exp_folders = [\n",
    "    os.path.join(PROJECT_ROOT, 'results', 'train_ranking_model', f)\n",
    "    for f in os.listdir(os.path.join(PROJECT_ROOT, 'results', 'train_ranking_model'))\n",
    "    ]\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    columns = [\n",
    "        'site', 'folder', 'data_file', 'min_month', 'max_month', 'min_hour', 'max_hour', 'num_train_pairs', 'num_eval_pairs', 'margin', 'margin_mode', 'augment', 'normalize', 'random_seed', 'min_val_loss', 'min_val_loss_epoch'\n",
    "    ]\n",
    ")\n",
    "\n",
    "for exp_folder in tqdm(exp_folders):\n",
    "    exp_params_file = os.path.join(exp_folder, 'params.txt')\n",
    "    exp_args = parse_configargparse_args(exp_params_file)\n",
    "    exp_metrics_file = os.path.join(\n",
    "        exp_folder, \n",
    "        f'metrics_per_epoch_ranking_margin_{exp_args[\"margin\"]}_randompairs_{exp_args[\"num_train_pairs\"]}_{exp_args[\"site\"]}_' + \\\n",
    "            ('augment_' if exp_args['augment'] else '') + \\\n",
    "            ('normalize_' if exp_args['normalize'] else '') + \\\n",
    "            f'{exp_args[\"random_seed\"]}.json'\n",
    "    )\n",
    "    exp_metrics = json.load(open(exp_metrics_file, 'r'))\n",
    "    min_val_loss = np.min(exp_metrics['val_loss'])\n",
    "    min_val_loss_epoch = np.argmin(exp_metrics['val_loss'])\n",
    "    df = pd.concat(\n",
    "        [df, pd.DataFrame([exp_args | {'folder': exp_folder, 'min_val_loss': min_val_loss, 'min_val_loss_epoch': min_val_loss_epoch}])],\n",
    "        ignore_index=True\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site</th>\n",
       "      <th>folder</th>\n",
       "      <th>data_file</th>\n",
       "      <th>min_month</th>\n",
       "      <th>max_month</th>\n",
       "      <th>min_hour</th>\n",
       "      <th>max_hour</th>\n",
       "      <th>num_train_pairs</th>\n",
       "      <th>num_eval_pairs</th>\n",
       "      <th>margin</th>\n",
       "      <th>...</th>\n",
       "      <th>min_val_loss_epoch</th>\n",
       "      <th>c</th>\n",
       "      <th>image_root_dir</th>\n",
       "      <th>gpu</th>\n",
       "      <th>output_root_dir</th>\n",
       "      <th>epochs</th>\n",
       "      <th>col_timestamp</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>lr</th>\n",
       "      <th>unfreeze_after</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AVERYBB</td>\n",
       "      <td>/home/amritagupta/ssdprivate/repos/fpe-model/r...</td>\n",
       "      <td>../data/raw/Avery_Brook_Bridge_01171000/flow-i...</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>18</td>\n",
       "      <td>5000</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>16</td>\n",
       "      <td>../conf/ranking.yml</td>\n",
       "      <td>../data/raw/Avery_Brook_Bridge_01171000/images/</td>\n",
       "      <td>1.0</td>\n",
       "      <td>../results/train_ranking_model</td>\n",
       "      <td>30.0</td>\n",
       "      <td>timestamp</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AVERYBB</td>\n",
       "      <td>/home/amritagupta/ssdprivate/repos/fpe-model/r...</td>\n",
       "      <td>../data/raw/Avery_Brook_Bridge_01171000/flow-i...</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>18</td>\n",
       "      <td>5000</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>14</td>\n",
       "      <td>../conf/ranking.yml</td>\n",
       "      <td>../data/raw/Avery_Brook_Bridge_01171000/images/</td>\n",
       "      <td>4.0</td>\n",
       "      <td>../results/train_ranking_model</td>\n",
       "      <td>30.0</td>\n",
       "      <td>timestamp</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WESTB0</td>\n",
       "      <td>/home/amritagupta/ssdprivate/repos/fpe-model/r...</td>\n",
       "      <td>../data/raw/West_Brook_0_Master_01171100/flow-...</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>18</td>\n",
       "      <td>5000</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>...</td>\n",
       "      <td>17</td>\n",
       "      <td>../conf/ranking.yml</td>\n",
       "      <td>../data/raw/West_Brook_0_Master_01171100/images/</td>\n",
       "      <td>1.0</td>\n",
       "      <td>../results/train_ranking_model</td>\n",
       "      <td>30.0</td>\n",
       "      <td>timestamp</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WESTB0</td>\n",
       "      <td>/home/amritagupta/ssdprivate/repos/fpe-model/r...</td>\n",
       "      <td>../data/raw/West_Brook_0_Master_01171100/flow-...</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>18</td>\n",
       "      <td>5000</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>../conf/ranking.yml</td>\n",
       "      <td>../data/raw/West_Brook_0_Master_01171100/images/</td>\n",
       "      <td>4.0</td>\n",
       "      <td>../results/train_ranking_model</td>\n",
       "      <td>30.0</td>\n",
       "      <td>timestamp</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      site                                             folder  \\\n",
       "0  AVERYBB  /home/amritagupta/ssdprivate/repos/fpe-model/r...   \n",
       "3  AVERYBB  /home/amritagupta/ssdprivate/repos/fpe-model/r...   \n",
       "1   WESTB0  /home/amritagupta/ssdprivate/repos/fpe-model/r...   \n",
       "2   WESTB0  /home/amritagupta/ssdprivate/repos/fpe-model/r...   \n",
       "\n",
       "                                           data_file min_month max_month  \\\n",
       "0  ../data/raw/Avery_Brook_Bridge_01171000/flow-i...         4        11   \n",
       "3  ../data/raw/Avery_Brook_Bridge_01171000/flow-i...         4        11   \n",
       "1  ../data/raw/West_Brook_0_Master_01171100/flow-...         4        11   \n",
       "2  ../data/raw/West_Brook_0_Master_01171100/flow-...         4        11   \n",
       "\n",
       "  min_hour max_hour num_train_pairs num_eval_pairs  margin  ...  \\\n",
       "0        7       18            5000           1000     0.1  ...   \n",
       "3        7       18            5000           1000     0.0  ...   \n",
       "1        7       18            5000           1000     0.1  ...   \n",
       "2        7       18            5000           1000     0.0  ...   \n",
       "\n",
       "  min_val_loss_epoch                    c  \\\n",
       "0                 16  ../conf/ranking.yml   \n",
       "3                 14  ../conf/ranking.yml   \n",
       "1                 17  ../conf/ranking.yml   \n",
       "2                  7  ../conf/ranking.yml   \n",
       "\n",
       "                                     image_root_dir  gpu  \\\n",
       "0   ../data/raw/Avery_Brook_Bridge_01171000/images/  1.0   \n",
       "3   ../data/raw/Avery_Brook_Bridge_01171000/images/  4.0   \n",
       "1  ../data/raw/West_Brook_0_Master_01171100/images/  1.0   \n",
       "2  ../data/raw/West_Brook_0_Master_01171100/images/  4.0   \n",
       "\n",
       "                  output_root_dir epochs col_timestamp batch_size     lr  \\\n",
       "0  ../results/train_ranking_model   30.0     timestamp       64.0  0.001   \n",
       "3  ../results/train_ranking_model   30.0     timestamp       64.0  0.001   \n",
       "1  ../results/train_ranking_model   30.0     timestamp       64.0  0.001   \n",
       "2  ../results/train_ranking_model   30.0     timestamp       64.0  0.001   \n",
       "\n",
       "  unfreeze_after  \n",
       "0            2.0  \n",
       "3            2.0  \n",
       "1            2.0  \n",
       "2            2.0  \n",
       "\n",
       "[4 rows x 25 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sort_values(by=['site', 'min_val_loss'])#[['site', 'folder', 'margin', 'min_val_loss', 'min_val_loss_epoch']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error: no \"view\" mailcap rules found for type \"image/png\"\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "img_path = '../data/raw/Avery_Brook_Bridge_01171000/images/Avery Bridge Downstream__2021-04-21__19-15-00(1).JPG'\n",
    "img = Image.open(img_path)\n",
    "img.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load a table of flow images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 44832 entries, 0 to 44831\n",
      "Data columns (total 8 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   station_name  44832 non-null  object \n",
      " 1   station_id    44832 non-null  int64  \n",
      " 2   imageset_id   44832 non-null  int64  \n",
      " 3   image_id      44832 non-null  int64  \n",
      " 4   timestamp     44832 non-null  object \n",
      " 5   filename      44832 non-null  object \n",
      " 6   url           44832 non-null  object \n",
      " 7   flow_cfs      44832 non-null  float64\n",
      "dtypes: float64(1), int64(3), object(4)\n",
      "memory usage: 2.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\n",
    "    '../../../data/Streamflow/sites/Avery_Brook_Bridge_01171000/flow-images.csv'\n",
    ")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(col_timestamp='timestamp'):\n",
    "    df = pd.read_csv(\n",
    "        '../../../data/Streamflow/sites/Avery_Brook_Bridge_01171000/flow-images.csv'\n",
    "    )\n",
    "    df[col_timestamp] = pd.to_datetime(df[col_timestamp])\n",
    "    df.sort_values(by=col_timestamp, inplace=True, ignore_index=True)\n",
    "    return df\n",
    "\n",
    "def convert_timezone(df, time_zone, col_timestamp='timestamp'):\n",
    "    df[col_timestamp] = df[col_timestamp].dt.tz_convert(tz=time_zone)\n",
    "    return df\n",
    "\n",
    "def filter_detections(detection_results, confidence_threshold: float, categories=[]):\n",
    "    \"\"\"Filter detections by confidence threshold and category.\n",
    "\n",
    "    Args:\n",
    "        detection_results: A dict containing MegaDetector v5 results.\n",
    "        confidence_threshold: A float representing the confidence below\n",
    "          which detections should be filtered out.\n",
    "        categories: A list of categories of detections to return. Detections\n",
    "          in other categories will be filtered out.\n",
    "\n",
    "    Returns:\n",
    "        A dict containing only MegaDetector v5 detection results above the\n",
    "        specified confidence threshold and belonging to the specified\n",
    "        categories.\n",
    "\n",
    "    Raises:\n",
    "\n",
    "    \"\"\"\n",
    "    filtered_results = copy.deepcopy(detection_results)\n",
    "    for image in tqdm(filtered_results[\"images\"]):\n",
    "        # keep only detections above confidence_threshold\n",
    "        # and of the specified categories\n",
    "        image[\"detections\"] = [\n",
    "            det\n",
    "            for det in image[\"detections\"]\n",
    "            if (det[\"conf\"] >= confidence_threshold) and (det[\"category\"] in categories)\n",
    "        ]\n",
    "        image[\"max_detection_conf\"] = (\n",
    "            max([det[\"conf\"] for det in image[\"detections\"]])\n",
    "            if len(image[\"detections\"]) > 0\n",
    "            else 0.0\n",
    "        )\n",
    "\n",
    "    # keep only images that have at least 1 detection after filtering\n",
    "    filtered_results[\"images\"] = [\n",
    "        image for image in filtered_results[\"images\"] if len(image[\"detections\"]) > 0\n",
    "    ]\n",
    "    return filtered_results\n",
    "\n",
    "def drop_by_col_val(df, col_val, col_name='filename'):\n",
    "    df = df[~df[col_name].isin(col_val)]\n",
    "    return df\n",
    "\n",
    "def filter_by_hour(df, min_hour=7, max_hour=18, col_timestamp='timestamp'):\n",
    "    df = df[\n",
    "        df[col_timestamp].dt.hour.between(min_hour, max_hour)\n",
    "    ]\n",
    "    return df\n",
    "\n",
    "def filter_by_month(df, min_month=4, max_month=11, col_timestamp='timestamp'):\n",
    "    df = df[\n",
    "        df[col_timestamp].dt.month.between(min_month, max_month)\n",
    "    ]\n",
    "    return df\n",
    "\n",
    "# def filter_by_date(self, start_date, end_date, mode=\"exclude\"):\n",
    "#     if mode == \"exclude\":\n",
    "#         before_start_date = self.data[self.col_timestamp] < start_date\n",
    "#         after_end_date = self.data[self.col_timestamp] > end_date\n",
    "#         outside_two_dates = before_start_date | after_end_date\n",
    "#         filtered_dates = self.data.loc[outside_two_dates].copy()\n",
    "#         self.data = filtered_dates\n",
    "#     else:\n",
    "#         raise NotImplementedError(\n",
    "#             'Please select \"exclude\" mode and provide date range to exclude.'\n",
    "#         )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 44828/44828 [00:00<00:00, 1392891.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df = load_data().pipe(convert_timezone, 'US/Eastern')\n",
    "\n",
    "pii_detection_results = json.load(\n",
    "    open('../results/pii_detection/md_v5a_Avery_Brook_Bridge_01171000_output.json', \"r\")\n",
    ")\n",
    "pii_detections = filter_detections(pii_detection_results, 0.2, [\"2\", \"3\"])[\"images\"]\n",
    "pii_files = pd.DataFrame(pii_detections)[\"file\"].tolist()\n",
    "df = df.pipe(drop_by_col_val, pii_files, col_name='filename')\n",
    "\n",
    "df = (\n",
    "    df.pipe(filter_by_hour, min_hour=6, max_hour=18)\n",
    "    .pipe(filter_by_month, min_month=3, max_month=11)\n",
    ").reset_index()\n",
    "print(len(df))\n",
    "# df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([17548, 17549, 17550, 17551, 17552])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.index.values[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "args:\n",
      "  data_file: /home/amritagupta/ssdprivate/data/Streamflow/sites/Avery_Brook_Bridge_01171000/flow-images.csv\n",
      "  image_dir: /home/amritagupta/ssdprivate/data/Streamflow/sites/Avery_Brook_Bridge_01171000/images\n",
      "  normalize: False\n",
      "  augment: True\n",
      "  batch_size: 64\n",
      "  pii_detection_results: /home/amritagupta/ssdprivate/repos/fpe-model/results/pii_detection/md_v5a_Avery_Brook_Bridge_01171000_output.json\n",
      "  seed: 939\n"
     ]
    }
   ],
   "source": [
    "args = Namespace(\n",
    "    data_file = '/home/amritagupta/ssdprivate/data/Streamflow/sites/Avery_Brook_Bridge_01171000/flow-images.csv',\n",
    "    image_dir = '/home/amritagupta/ssdprivate/data/Streamflow/sites/Avery_Brook_Bridge_01171000/images',\n",
    "    normalize = False,\n",
    "    augment = True,\n",
    "    batch_size = 64,\n",
    "    pii_detection_results  = '/home/amritagupta/ssdprivate/repos/fpe-model/results/pii_detection/md_v5a_Avery_Brook_Bridge_01171000_output.json',\n",
    "    seed = 939\n",
    ")\n",
    "print(\"args:\")\n",
    "for arg in vars(args):\n",
    "    print(f\"  {arg}: {getattr(args, arg)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_data_file(filepath, pii_detections):\n",
    "    # logger.info(f\"load dataset: {filepath}\")\n",
    "    df = pd.read_csv(filepath, dtype={\"flow_cfs\": np.float32})\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"]).dt.tz_convert(tz=\"US/Eastern\")\n",
    "    df.sort_values(by=\"timestamp\", inplace=True, ignore_index=True)\n",
    "\n",
    "    # filter by hour\n",
    "    min_hour = 6\n",
    "    max_hour = 18\n",
    "    # logger.info(f\"filter(hour): {min_hour} to {max_hour}\")\n",
    "    df = df[df[\"timestamp\"].dt.hour.between(min_hour, max_hour)]\n",
    "\n",
    "    # filter by month\n",
    "    min_month = 3\n",
    "    max_month = 11\n",
    "    # logger.info(f\"filter(month): {min_month} to {max_month}\")\n",
    "    df = df[df[\"timestamp\"].dt.month.between(min_month, max_month)]\n",
    "\n",
    "    # filter by pii detections\n",
    "    pii_results = json.load(open(args.pii_detection_results, \"r\"))\n",
    "    pii_detections = filter_detections(pii_results, 0.2, [\"2\", \"3\"])[\"images\"]\n",
    "    pii_files = pd.DataFrame(pii_detections)[\"file\"].tolist()\n",
    "    df = df[~df[\"filename\"].isin(pii_files)]\n",
    "\n",
    "    # logger.info(\n",
    "    #     f\"dataset loaded\\n  rows: {len(df)}\\n  flow: {df.flow_cfs.mean():>.2f} cfs\"\n",
    "    # )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # DATALOADING OPTS\n",
    "# def data_args(parser):\n",
    "#     group = parser.add_argument_group(\n",
    "#         \"Data\", \"Arguments control Data and loading for training\"\n",
    "#     )\n",
    "#     group.add_argument(\n",
    "#         \"--data-file\",\n",
    "#         type=str,\n",
    "#         required=True,\n",
    "#         help=\"path to CSV file with linked images and flows\",\n",
    "#     )\n",
    "#     group.add_argument(\n",
    "#         \"--image-dir\",\n",
    "#         type=str,\n",
    "#         required=True,\n",
    "#         help=\"path to folder containing images listed in data-file\",\n",
    "#     )\n",
    "#     # group.add_argument(\n",
    "#     #     \"--split-idx\",\n",
    "#     #     type=int,\n",
    "#     #     required=True,\n",
    "#     #     help=\"index specifying which of 5 train/val splits to use\",\n",
    "#     # )\n",
    "#     group.add_argument(\n",
    "#         \"--normalize\",\n",
    "#         type=bool,\n",
    "#         default=True,\n",
    "#         help=\"whether to normalize image inputs to model\",\n",
    "#     )\n",
    "#     group.add_argument(\n",
    "#         \"--augment\",\n",
    "#         type=bool,\n",
    "#         default=True,\n",
    "#         help=\"whether to use image augmentation during training\",\n",
    "#     )\n",
    "#     # group.add_argument(\n",
    "#     #     \"--crop-to-bbox\",\n",
    "#     #     type=bool,\n",
    "#     #     default=False,\n",
    "#     #     help=\"whether to crop images to bounding boxes before training\",\n",
    "#     # )\n",
    "#     group.add_argument(\n",
    "#         \"--batch-size\", type=int, default=64, help=\"batch size of the train loader\"\n",
    "#     )\n",
    "\n",
    "# def get_args():\n",
    "#     parser = argparse.ArgumentParser(description=\"\")\n",
    "#     parser.add_argument(\"--seed\", default=939, type=int, help=\"random seed\")\n",
    "#     data_args(parser)\n",
    "#     # add temporary arguments that should be refactored out\n",
    "#     parser.add_argument(\"--pii-detection-results\", default=None)\n",
    "#     args = parser.parse_args()\n",
    "#     return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 44828/44828 [00:00<00:00, 776553.58it/s]\n",
      "100%|██████████| 1000/1000 [01:26<00:00, 11.61it/s]\n"
     ]
    }
   ],
   "source": [
    "args = args #get_args()\n",
    "\n",
    "# Load data\n",
    "df = _load_data_file(args.data_file, args.pii_detection_results)\n",
    "# NOTE: Dataset Splitter class is incomplete\n",
    "# NOTE: Fragile to return indices of dataset copy sorted in function call\n",
    "train_inds, val_inds, test_inds = RandomStratifiedWeeklyFlow().split(\n",
    "    df, 0.8, 0.1, 0.1\n",
    ")\n",
    "train_df, val_df, test_df = (\n",
    "    df.iloc[train_inds],\n",
    "    df.iloc[val_inds],\n",
    "    df.iloc[test_inds],\n",
    ")\n",
    "\n",
    "# Create PyTorch Datsets\n",
    "train_ds_tmp = FlowPhotoDataset(train_df, os.path.dirname(args.image_dir))\n",
    "train_mean, train_std = train_ds_tmp.compute_mean_std()\n",
    "image = train_ds_tmp.get_image(0)\n",
    "aspect = image.shape[2] / image.shape[1]\n",
    "\n",
    "train_transforms = [Resize([480, np.int32(480 * aspect)])]\n",
    "if args.augment:\n",
    "    train_transforms.append(RandomCrop([384, np.int32(384 * aspect)]))\n",
    "    train_transforms.append(RandomHorizontalFlip())\n",
    "    train_transforms.append(RandomRotation(10))\n",
    "    train_transforms.append(ColorJitter())\n",
    "else:\n",
    "    train_transforms.append(CenterCrop([384, np.int32(384 * aspect)]))\n",
    "# train_transforms.append(ToTensor())\n",
    "if args.normalize:\n",
    "    train_transforms.append(Normalize(train_mean, train_std))\n",
    "train_transform = Compose(train_transforms)\n",
    "train_ds = FlowPhotoDataset(\n",
    "    train_df, os.path.dirname(args.image_dir), transform=train_transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not read image index 12302\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'image' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/amritagupta/ssdprivate/repos/fpe-model/scripts/Scratch.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bdsnahpc/home/amritagupta/ssdprivate/repos/fpe-model/scripts/Scratch.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m image \u001b[39m=\u001b[39m train_ds\u001b[39m.\u001b[39;49mget_image(\u001b[39m12302\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdsnahpc/home/amritagupta/ssdprivate/repos/fpe-model/scripts/Scratch.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mshow_image\u001b[39m(image):\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdsnahpc/home/amritagupta/ssdprivate/repos/fpe-model/scripts/Scratch.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     image_pil \u001b[39m=\u001b[39m ToPILImage()(image)\n",
      "File \u001b[0;32m~/ssdprivate/repos/fpe-model/scripts/../src/datasets.py:96\u001b[0m, in \u001b[0;36mFlowPhotoDataset.get_image\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m     95\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCould not read image index \u001b[39m\u001b[39m{\u001b[39;00mindex\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 96\u001b[0m image \u001b[39m=\u001b[39m image \u001b[39m/\u001b[39m \u001b[39m255.0\u001b[39m  \u001b[39m# convert to float in [0,1]\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[39mreturn\u001b[39;00m image\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'image' referenced before assignment"
     ]
    }
   ],
   "source": [
    "image = train_ds.get_image(12302)\n",
    "\n",
    "def show_image(image):\n",
    "    image_pil = ToPILImage()(image)\n",
    "    display(image_pil)\n",
    "    \n",
    "show_image(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://usgs-chs-conte-prod-fpe-storage.s3.amazonaws.com/imagesets/b9f642d4-027d-4d11-acde-1ae889fd2ecb/images/Avery Brook Bridge__2022-10-28__08-15-00(1).JPG'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds.table.iloc[12302].url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Image is incomplete or truncated",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/amritagupta/ssdprivate/repos/fpe-model/scripts/Scratch.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bdsnahpc/home/amritagupta/ssdprivate/repos/fpe-model/scripts/Scratch.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m im, label \u001b[39m=\u001b[39m train_ds[\u001b[39m12302\u001b[39;49m]\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdsnahpc/home/amritagupta/ssdprivate/repos/fpe-model/scripts/Scratch.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m show_image(im)\n",
      "File \u001b[0;32m~/ssdprivate/repos/fpe-model/scripts/../src/datasets.py:96\u001b[0m, in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m     95\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCould not read image index \u001b[39m\u001b[39m{\u001b[39;00mindex\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 96\u001b[0m image \u001b[39m=\u001b[39m image \u001b[39m/\u001b[39m \u001b[39m255.0\u001b[39m  \u001b[39m# convert to float in [0,1]\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[39mreturn\u001b[39;00m image\n",
      "File \u001b[0;32m~/ssdprivate/repos/fpe-model/scripts/../src/datasets.py:91\u001b[0m, in \u001b[0;36mget_image\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_image\u001b[39m(\u001b[39mself\u001b[39m, index):\n\u001b[1;32m     88\u001b[0m     filename \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtable\u001b[39m.\u001b[39miloc[index][\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcol_filename]\n\u001b[1;32m     89\u001b[0m     image_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\n\u001b[1;32m     90\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_dir, \u001b[39m\"\u001b[39m\u001b[39mimages\u001b[39m\u001b[39m\"\u001b[39m, filename\n\u001b[0;32m---> 91\u001b[0m     )  \u001b[39m# TODO: see if we can avoid secret subdirs\u001b[39;00m\n\u001b[1;32m     92\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     93\u001b[0m         image \u001b[39m=\u001b[39m read_image(image_path)\n",
      "File \u001b[0;32m~/.conda/envs/streamflow/lib/python3.10/site-packages/torchvision/io/image.py:254\u001b[0m, in \u001b[0;36mread_image\u001b[0;34m(path, mode)\u001b[0m\n\u001b[1;32m    252\u001b[0m     _log_api_usage_once(read_image)\n\u001b[1;32m    253\u001b[0m data \u001b[39m=\u001b[39m read_file(path)\n\u001b[0;32m--> 254\u001b[0m \u001b[39mreturn\u001b[39;00m decode_image(data, mode)\n",
      "File \u001b[0;32m~/.conda/envs/streamflow/lib/python3.10/site-packages/torchvision/io/image.py:231\u001b[0m, in \u001b[0;36mdecode_image\u001b[0;34m(input, mode)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mis_scripting() \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mis_tracing():\n\u001b[1;32m    230\u001b[0m     _log_api_usage_once(decode_image)\n\u001b[0;32m--> 231\u001b[0m output \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mops\u001b[39m.\u001b[39;49mimage\u001b[39m.\u001b[39;49mdecode_image(\u001b[39minput\u001b[39;49m, mode\u001b[39m.\u001b[39;49mvalue)\n\u001b[1;32m    232\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/.conda/envs/streamflow/lib/python3.10/site-packages/torch/_ops.py:442\u001b[0m, in \u001b[0;36mOpOverloadPacket.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    438\u001b[0m     \u001b[39m# overloading __call__ to ensure torch.ops.foo.bar()\u001b[39;00m\n\u001b[1;32m    439\u001b[0m     \u001b[39m# is still callable from JIT\u001b[39;00m\n\u001b[1;32m    440\u001b[0m     \u001b[39m# We save the function ptr as the `op` attribute on\u001b[39;00m\n\u001b[1;32m    441\u001b[0m     \u001b[39m# OpOverloadPacket to access it here.\u001b[39;00m\n\u001b[0;32m--> 442\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_op(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs \u001b[39mor\u001b[39;49;00m {})\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Image is incomplete or truncated"
     ]
    }
   ],
   "source": [
    "im, label = train_ds[12302]\n",
    "show_image(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.49695178 0.50054886 0.47154651]\n",
      "[0.34803897 0.346689   0.36952244]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# stat = ImageStat.Stat(ToPILImage()(image))\n",
    "# print(np.array(stat.mean) / 255.0)\n",
    "# print(np.array(stat.stddev) / 255.0)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "max",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/amritagupta/ssdprivate/repos/fpe-model/scripts/Scratch.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bdsnahpc/home/amritagupta/ssdprivate/repos/fpe-model/scripts/Scratch.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m pilimage\u001b[39m.\u001b[39;49mmax()\n",
      "File \u001b[0;32m~/.conda/envs/streamflow/lib/python3.10/site-packages/PIL/Image.py:529\u001b[0m, in \u001b[0;36mImage.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    527\u001b[0m     deprecate(\u001b[39m\"\u001b[39m\u001b[39mImage categories\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m10\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mis_animated\u001b[39m\u001b[39m\"\u001b[39m, plural\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    528\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_category\n\u001b[0;32m--> 529\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(name)\n",
      "\u001b[0;31mAttributeError\u001b[0m: max"
     ]
    }
   ],
   "source": [
    "pilimage.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 44828/44828 [00:00<00:00, 587540.18it/s]\n"
     ]
    }
   ],
   "source": [
    "def filter_detections(\n",
    "    detection_results,\n",
    "    confidence_threshold: float,\n",
    "    categories = []\n",
    "):\n",
    "    \"\"\"Filter detections by confidence threshold and category.\n",
    "    \n",
    "    Args:\n",
    "        detection_results: A dict containing MegaDetector v5 results.\n",
    "        confidence_threshold: A float representing the confidence below\n",
    "          which detections should be filtered out.\n",
    "        categories: A list of categories of detections to return. Detections\n",
    "          in other categories will be filtered out.\n",
    "\n",
    "    Returns:\n",
    "        A dict containing only MegaDetector v5 detection results above the\n",
    "        specified confidence threshold and belonging to the specified \n",
    "        categories.\n",
    "\n",
    "    Raises:\n",
    "        \n",
    "    \"\"\"\n",
    "    filtered_results = copy.deepcopy(detection_results)\n",
    "    for image in tqdm(filtered_results['images']):\n",
    "        # keep only detections above confidence_threshold\n",
    "        # and of the specified categories\n",
    "        image['detections'] = [\n",
    "            det for det in image['detections']\n",
    "            if (det['conf'] >= confidence_threshold)\n",
    "            and (det['category'] in categories)\n",
    "        ]\n",
    "        image['max_detection_conf'] = max([\n",
    "            det['conf'] for det in image['detections']\n",
    "        ]) if len(image['detections']) > 0 else 0.0\n",
    "    \n",
    "    # keep only images that have at least 1 detection after filtering\n",
    "    filtered_results['images'] = [\n",
    "        image for image in filtered_results['images']\n",
    "        if len(image['detections']) > 0\n",
    "    ]\n",
    "    return filtered_results\n",
    "pii_results = json.load(open(args.pii_filename, 'r'))\n",
    "pii_detections = filter_detections(pii_results, 0.2, ['2', '3'])['images']\n",
    "pii_files = pd.DataFrame(pii_detections)['file'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44832\n",
      "44777\n",
      "22399\n",
      "dataset loaded\n",
      "  rows: 22399\n",
      "  flow: 5.61 cfs\n"
     ]
    }
   ],
   "source": [
    "# load raw data file\n",
    "site_df = pd.read_csv(os.path.join(args.data_dir, args.filename), dtype={\"flow_cfs\": np.float32})\n",
    "site_df[\"timestamp\"] = pd.to_datetime(site_df[\"timestamp\"]).dt.tz_convert(tz=\"US/Eastern\")\n",
    "print(len(site_df))\n",
    "\n",
    "# Until a standard filtering procedure is decided upon, filtering by\n",
    "#  pii detection/month of year/hour of day/specific date ranges/etc.\n",
    "#  should be done prior to initializing the dataset (for transparency)\n",
    "\n",
    "# filter out pii\n",
    "site_df = site_df[~site_df['filename'].isin(pii_files)]\n",
    "print(len(site_df))\n",
    "\n",
    "# filter by hour\n",
    "min_hour = 7\n",
    "max_hour = 18\n",
    "# logger.info(f\"filter(hour): {min_hour} to {max_hour}\")\n",
    "site_df = site_df[site_df[\"timestamp\"].dt.hour.between(min_hour, max_hour)]\n",
    "print(len(site_df))\n",
    "\n",
    "# min_month = 4\n",
    "# max_month = 11\n",
    "# # logger.info(f\"filter(month): {min_month} to {max_month}\")\n",
    "# site_df = site_df[site_df[\"timestamp\"].dt.month.between(min_month, max_month)]\n",
    "\n",
    "print(f\"dataset loaded\\n  rows: {len(site_df)}\\n  flow: {site_df.flow_cfs.mean():>.2f} cfs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = RandomStratifiedWeeklyFlow()\n",
    "train_inds, val_inds, test_inds = splitter.split(site_df, 0.8, 0.1, 0.1)\n",
    "train_df = site_df.iloc[train_inds]\n",
    "val_df = site_df.iloc[val_inds]\n",
    "test_df = site_df.iloc[test_inds]\n",
    "\n",
    "assert set(train_inds).intersection(set(test_inds)) == set(), 'train and test contain overlapping samples'\n",
    "assert set(train_inds).intersection(set(val_inds)) == set(), 'train and val contain overlapping samples'\n",
    "assert set(test_inds).intersection(set(val_inds)) == set(), 'test and val contain overlapping samples'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:05<00:00, 15.29it/s]\n"
     ]
    }
   ],
   "source": [
    "# create transforms\n",
    "from torchvision.io import read_image\n",
    "from torchvision import transforms\n",
    "from PIL import ImageStat\n",
    "\n",
    "# compute mean and std from train set\n",
    "train_ds = FlowPhotoDataset(train_df, '/home/amritagupta/ssdprivate/data/Streamflow/sites/Avery_Brook_Bridge_01171000')\n",
    "\n",
    "def compute_mean_stddev(dataset, max_sample_size=1000):\n",
    "    means = np.zeros((3))\n",
    "    stddevs = np.zeros((3))\n",
    "    sample_size = min(len(train_ds.table), max_sample_size)\n",
    "    sample_indices = np.random.choice(\n",
    "        len(dataset.table), size=sample_size, replace=False\n",
    "    )\n",
    "    for index in tqdm(sample_indices):\n",
    "        image, _ = dataset[index]\n",
    "        image = transforms.functional.to_pil_image(image)\n",
    "        stat = ImageStat.Stat(image)\n",
    "        means += np.array(stat.mean) / 255.0\n",
    "        stddevs += np.array(stat.stddev) / 255.0\n",
    "    means = means / sample_size\n",
    "    stddevs = stddevs / sample_size\n",
    "    return means, stddevs\n",
    "\n",
    "means, stddevs = compute_mean_stddev(train_ds)\n",
    "\n",
    "# compute aspect ratio\n",
    "image, _ = train_ds[0]\n",
    "aspect = image.shape[2] / image.shape[1]\n",
    "\n",
    "\n",
    "    \n",
    "# img_path = os.path.join(args.data_dir, train_df['filename'].iloc[0])\n",
    "# # img_dir = os.path.join(data_dir, \"images\")\n",
    "# # img_path = os.path.join(img_dir, train_df[\"filename\"].iloc[0])\n",
    "# print(f\"loading first image: {img_path}\")\n",
    "# img = read_image(img_path)\n",
    "# aspect = img.shape[2] / img.shape[1]\n",
    "# print(aspect)\n",
    "# # train_ds = FlowPhotoRegressionDataset(train_df, '/home/amritagupta/ssdprivate/data/Streamflow/sites/Avery_Brook_Bridge_01171000')\n",
    "# train_transforms = [\n",
    "\n",
    "# ]\n",
    "# # read first image\n",
    "# # m, s = train_ds.compute_mean_std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_AUGMENTATION = False\n",
    "NORMALIZATION = True\n",
    "\n",
    "train_transforms = [\n",
    "    transforms.Resize([480, np.int32(480 * aspect)]),\n",
    "]\n",
    "if DATA_AUGMENTATION:\n",
    "    train_transforms.append(transforms.RandomCrop([384, np.int32(384 * aspect)]))\n",
    "    train_transforms.append(transforms.RandomHorizontalFlip())\n",
    "    train_transforms.append(transforms.RandomRotation(10))\n",
    "    train_transforms.append(transforms.ColorJitter())\n",
    "else:\n",
    "    train_transforms.append(transforms.CenterCrop([384, np.int32(384 * aspect)]))\n",
    "train_transforms.append(transforms.ToTensor())\n",
    "if NORMALIZATION:\n",
    "    train_transforms.append(transforms.Normalize(means, stddevs))\n",
    "train_transform = transforms.Compose(train_transforms)\n",
    "\n",
    "eval_transforms = [\n",
    "    transforms.Resize([480, np.int32(480 * aspect)]),\n",
    "    transforms.CenterCrop([384, np.int32(384 * aspect)]),\n",
    "    transforms.ToTensor()\n",
    "]\n",
    "if NORMALIZATION:\n",
    "    eval_transforms.append(transforms.Normalize(means, stddevs))\n",
    "eval_transform = transforms.Compose(eval_transforms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = FlowPhotoDataset(\n",
    "    train_df,\n",
    "    '/home/amritagupta/ssdprivate/data/Streamflow/sites/Avery_Brook_Bridge_01171000/',\n",
    "    transform=train_transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Unexpected type <class 'numpy.ndarray'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/amritagupta/ssdprivate/repos/fpe-model/scripts/Scratch.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bdsnahpc/home/amritagupta/ssdprivate/repos/fpe-model/scripts/Scratch.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m train_ds[\u001b[39m0\u001b[39;49m]\n",
      "File \u001b[0;32m~/ssdprivate/repos/fpe-model/scripts/../src/datasets.py:120\u001b[0m, in \u001b[0;36mFlowPhotoDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    118\u001b[0m image \u001b[39m=\u001b[39m image \u001b[39m/\u001b[39m \u001b[39m255.0\u001b[39m  \u001b[39m# convert to float in [0,1]\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform:\n\u001b[0;32m--> 120\u001b[0m     image \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(\n\u001b[1;32m    121\u001b[0m         image\u001b[39m.\u001b[39;49mnumpy()\n\u001b[1;32m    122\u001b[0m     )  \u001b[39m# without converting to numpy or reading as PILImage, transforms don't work\u001b[39;00m\n\u001b[1;32m    123\u001b[0m label \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtable\u001b[39m.\u001b[39miloc[index][\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcol_label]\n\u001b[1;32m    124\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabel_transform:\n",
      "File \u001b[0;32m~/.conda/envs/streamflow/lib/python3.10/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[39m=\u001b[39m t(img)\n\u001b[1;32m     96\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/.conda/envs/streamflow/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/streamflow/lib/python3.10/site-packages/torchvision/transforms/transforms.py:346\u001b[0m, in \u001b[0;36mResize.forward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[1;32m    339\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    340\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m    341\u001b[0m \u001b[39m        img (PIL Image or Tensor): Image to be scaled.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[39m        PIL Image or Tensor: Rescaled image.\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 346\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mresize(img, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msize, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minterpolation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_size, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mantialias)\n",
      "File \u001b[0;32m~/.conda/envs/streamflow/lib/python3.10/site-packages/torchvision/transforms/functional.py:462\u001b[0m, in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[39mif\u001b[39;00m max_size \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(size) \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    457\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    458\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mmax_size should only be passed if size specifies the length of the smaller edge, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    459\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mi.e. size should be an int or a sequence of length 1 in torchscript mode.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    460\u001b[0m         )\n\u001b[0;32m--> 462\u001b[0m _, image_height, image_width \u001b[39m=\u001b[39m get_dimensions(img)\n\u001b[1;32m    463\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(size, \u001b[39mint\u001b[39m):\n\u001b[1;32m    464\u001b[0m     size \u001b[39m=\u001b[39m [size]\n",
      "File \u001b[0;32m~/.conda/envs/streamflow/lib/python3.10/site-packages/torchvision/transforms/functional.py:75\u001b[0m, in \u001b[0;36mget_dimensions\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(img, torch\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m     73\u001b[0m     \u001b[39mreturn\u001b[39;00m F_t\u001b[39m.\u001b[39mget_dimensions(img)\n\u001b[0;32m---> 75\u001b[0m \u001b[39mreturn\u001b[39;00m F_pil\u001b[39m.\u001b[39;49mget_dimensions(img)\n",
      "File \u001b[0;32m~/.conda/envs/streamflow/lib/python3.10/site-packages/torchvision/transforms/functional_pil.py:33\u001b[0m, in \u001b[0;36mget_dimensions\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m     31\u001b[0m     width, height \u001b[39m=\u001b[39m img\u001b[39m.\u001b[39msize\n\u001b[1;32m     32\u001b[0m     \u001b[39mreturn\u001b[39;00m [channels, height, width]\n\u001b[0;32m---> 33\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnexpected type \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(img)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Unexpected type <class 'numpy.ndarray'>"
     ]
    }
   ],
   "source": [
    "train_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RGB'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "image_path = os.path.join(\n",
    "    '/home/amritagupta/ssdprivate/data/Streamflow/sites/Avery_Brook_Bridge_01171000/images',\n",
    "    train_df['filename'].iloc[0]\n",
    ")\n",
    "image = Image.open(image_path)\n",
    "image.mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[252, 252, 164],\n",
       "        [214, 211, 116],\n",
       "        [200, 190,  77],\n",
       "        ...,\n",
       "        [233, 252, 248],\n",
       "        [207, 243, 255],\n",
       "        [138, 180, 205]],\n",
       "\n",
       "       [[246, 255, 162],\n",
       "        [215, 220, 126],\n",
       "        [205, 200, 108],\n",
       "        ...,\n",
       "        [242, 255, 255],\n",
       "        [194, 225, 230],\n",
       "        [139, 175, 187]],\n",
       "\n",
       "       [[210, 225, 134],\n",
       "        [201, 210, 127],\n",
       "        [184, 187, 116],\n",
       "        ...,\n",
       "        [127, 141, 142],\n",
       "        [121, 141, 139],\n",
       "        [138, 163, 157]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[254, 255, 253],\n",
       "        [254, 255, 253],\n",
       "        [254, 255, 253],\n",
       "        ...,\n",
       "        [254, 255, 253],\n",
       "        [254, 255, 253],\n",
       "        [254, 255, 253]],\n",
       "\n",
       "       [[254, 255, 253],\n",
       "        [254, 255, 253],\n",
       "        [254, 255, 253],\n",
       "        ...,\n",
       "        [254, 255, 253],\n",
       "        [254, 255, 253],\n",
       "        [254, 255, 253]],\n",
       "\n",
       "       [[254, 255, 253],\n",
       "        [254, 255, 253],\n",
       "        [254, 255, 253],\n",
       "        ...,\n",
       "        [254, 255, 253],\n",
       "        [254, 255, 253],\n",
       "        [254, 255, 253]]], dtype=uint8)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.asarray(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "streamflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
