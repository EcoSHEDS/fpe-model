{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(sys.path[0], os.pardir))\n",
    "sys.path.append(PROJECT_ROOT)\n",
    "from src.datasets import classify3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load up some data\n",
    "path_to_data_file = '../../../data/Streamflow/fpe_stations/AVERYBB/AVERYBB-20230829/data/flow-images.csv'\n",
    "data = pd.read_csv(path_to_data_file)\n",
    "data['timestamp'] = pd.to_datetime(data['timestamp'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def add_time_segment_columns(data: pd.DataFrame, window_size: str, include_year: bool = True) -> pd.DataFrame:\n",
    "#     \"\"\"\n",
    "#     Adds columns to the DataFrame that represent the time segments (windows) that each row belongs to.\n",
    "\n",
    "#     Args:\n",
    "#         data (pd.DataFrame): The input DataFrame with a \"timestamp\" column.\n",
    "#         window_size (str): The size of the time segments (windows). Must be \"week\" or \"day\".\n",
    "#         include_year (bool): Whether or not to include the \"year\" column. Default is True.\n",
    "\n",
    "#     Returns:\n",
    "#         pd.DataFrame: The input DataFrame with additional columns \"window\" and \"year\" (if include_year is True) representing the time segments.\n",
    "\n",
    "#     Example:\n",
    "#         >>> data = pd.DataFrame({\n",
    "#         ...     \"timestamp\": pd.date_range(\"2022-01-01\", \"2022-01-31\", freq=\"H\"),\n",
    "#         ...     \"value\": np.random.randn(744)\n",
    "#         ... })\n",
    "#         >>> data = add_time_segment_columns(data, \"day\", include_year=True)\n",
    "#         >>> data.head()\n",
    "#                     timestamp     value  window  year\n",
    "#         0 2022-01-01 00:00:00 -0.051771       1  2022\n",
    "#         1 2022-01-01 01:00:00 -0.358163       1  2022\n",
    "#         2 2022-01-01 02:00:00 -0.080947       1  2022\n",
    "#         3 2022-01-01 03:00:00 -0.126756       1  2022\n",
    "#         4 2022-01-01 04:00:00 -0.401780       1  2022\n",
    "\n",
    "#     Raises:\n",
    "#         ValueError: If window_size is not \"week\" or \"day\".\n",
    "#     \"\"\"\n",
    "#     if window_size == \"week\":\n",
    "#         data[\"window\"] = data[\"timestamp\"].dt.isocalendar().week\n",
    "#     elif window_size == \"day\":\n",
    "#         data[\"window\"] = data[\"timestamp\"].dt.day\n",
    "#     else:\n",
    "#         raise ValueError(\"Window size must be 'week' or 'day'.\")\n",
    "    \n",
    "#     if include_year:\n",
    "#         data[\"year\"] = data[\"timestamp\"].dt.isocalendar().year\n",
    "    \n",
    "#     return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindowedDatasetSplitter:\n",
    "    \"\"\"\n",
    "    Splits a dataset into train, validation, and test sets based on the time segments (windows) of the data.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data: pd.DataFrame, window_size: str, include_year: bool = True):\n",
    "        \"\"\"\n",
    "        Initializes the WindowedDatasetSplitter class with the specified data, window size, and include_year flag.\n",
    "\n",
    "        Args:\n",
    "            data (pd.DataFrame): The input DataFrame with a \"timestamp\" column.\n",
    "            window_size (str): The size of the time segments (windows). Must be \"week\" or \"day\".\n",
    "            include_year (bool): Whether or not to include the \"year\" column. Default is True.\n",
    "\n",
    "        Example:\n",
    "            >>> data = pd.DataFrame({\n",
    "            ...     \"timestamp\": pd.date_range(\"2022-01-01\", \"2022-01-31\", freq=\"H\"),\n",
    "            ...     \"value\": np.random.randn(744)\n",
    "            ... })\n",
    "            >>> splitter = WindowedDatasetSplitter(data, \"day\", include_year=True)\n",
    "            >>> splitter.data.head()\n",
    "                        timestamp     value  window  year\n",
    "            0 2022-01-01 00:00:00 -0.051771       1  2022\n",
    "            1 2022-01-01 01:00:00 -0.358163       1  2022\n",
    "            2 2022-01-01 02:00:00 -0.080947       1  2022\n",
    "            3 2022-01-01 03:00:00 -0.126756       1  2022\n",
    "            4 2022-01-01 04:00:00 -0.401780       1  2022\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If window_size is not \"week\" or \"day\".\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.window_size = window_size\n",
    "        self.include_year = include_year\n",
    "        self._get_windows()\n",
    "    \n",
    "    def _get_windows(self):\n",
    "        if self.window_size == \"week\":\n",
    "            self.data[\"window\"] = self.data[\"timestamp\"].dt.isocalendar().week\n",
    "        elif self.window_size == \"day\":\n",
    "            self.data[\"window\"] = self.data[\"timestamp\"].dt.day\n",
    "        else:\n",
    "            raise ValueError(\"Window size must be 'week' or 'day'.\")\n",
    "        \n",
    "        if self.include_year:\n",
    "            self.data[\"year\"] = self.data[\"timestamp\"].dt.isocalendar().year\n",
    "    \n",
    "    def shuffle_split(self, train_size: float, val_size: float, test_size: float, random_state: int = 42):\n",
    "        \"\"\"\n",
    "        Randomly assigns the data time segments into train, validation, and test sets of specified sizes.\n",
    "\n",
    "        Args:\n",
    "            train_size (float): The size of the training set as a fraction of the total time segments in the dataset.\n",
    "            val_size (float): The size of the validation set as a fraction of the total time segments in the dataset.\n",
    "            test_size (float): The size of the test set as a fraction of the total time segments in the dataset.\n",
    "            random_state (int): The random state to use for the shuffle. Default is 42.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame, pd.DataFrame, pd.DataFrame: The train, validation, and test sets.\n",
    "\n",
    "        Example:\n",
    "            >>> data = pd.DataFrame({\n",
    "            ...     \"timestamp\": pd.date_range(\"2022-01-01\", \"2022-01-31\", freq=\"H\"),\n",
    "            ...     \"value\": np.random.randn(744)\n",
    "            ... })\n",
    "            >>> splitter = WindowedDatasetSplitter(data, \"day\", include_year=True)\n",
    "            >>> train, val, test = splitter.shuffle_split(0.7, 0.2, 0.1)\n",
    "            >>> train.shape\n",
    "            (520, 4)\n",
    "            >>> val.shape\n",
    "            (149, 4)\n",
    "            >>> test.shape\n",
    "            (75, 4)\n",
    "        \"\"\"\n",
    "        np.testing.assert_almost_equal(train_size + val_size + test_size, 1.0)\n",
    "        if self.include_year:\n",
    "            grouped = self.data.groupby([\"year\", \"window\"])\n",
    "        else:\n",
    "            grouped = self.data.groupby(\"window\")\n",
    "        windows = grouped.groups.keys()\n",
    "        windows = list(windows)\n",
    "        np.random.seed(random_state)\n",
    "        np.random.shuffle(windows)\n",
    "        train_windows = sorted(windows[:int(train_size * len(windows))])\n",
    "        val_windows = sorted(windows[int(train_size * len(windows)):int((train_size + val_size) * len(windows))])\n",
    "        test_windows = sorted(windows[int((train_size + val_size) * len(windows)):])\n",
    "        train = pd.concat([grouped.get_group(tuple(train_windows[i])) for i in range(len(train_windows))])\n",
    "        val = pd.concat([grouped.get_group(tuple(val_windows[i])) for i in range(len(val_windows))])\n",
    "        test = pd.concat([grouped.get_group(tuple(test_windows[i])) for i in range(len(test_windows))])\n",
    "        return {\n",
    "            \"train\": train,\n",
    "            \"val\": val,\n",
    "            \"test\": test\n",
    "        }\n",
    "    \n",
    "    def stratified_shuffle_split(self, train_size: float, val_size: float, test_size: float, stratify_by: str = \"flow_cfs\", random_state: int = 42):\n",
    "        np.testing.assert_almost_equal(train_size + val_size + test_size, 1.0)\n",
    "        df = self.data.copy()\n",
    "        df.sort_values(by=\"timestamp\", inplace=True, ignore_index=True)\n",
    "\n",
    "        # compute mean value for each time window\n",
    "        time_window_cols = [\"year\", \"window\"] if self.include_year else [\"window\"]\n",
    "        stratify_by_cols = [stratify_by] + time_window_cols\n",
    "        window_val_means = (\n",
    "            df[stratify_by_cols]\n",
    "            .groupby(time_window_cols)\n",
    "            .mean()\n",
    "            .rename(columns={stratify_by: \"mean_\"+stratify_by})\n",
    "        )\n",
    "        window_val_quantiles = np.quantile(\n",
    "            window_val_means[\"mean_\"+stratify_by].values, [0.25, 0.75], axis=0\n",
    "        )\n",
    "        window_val_means[\"value_class\"] = window_val_means[\"mean_\"+stratify_by].map(\n",
    "            lambda x: classify3(window_val_quantiles[0], window_val_quantiles[1], x)\n",
    "        )\n",
    "        window_val_means[\"window_index\"] = range(len(window_val_means.index))        \n",
    "\n",
    "        df = (\n",
    "            df.set_index(time_window_cols)\n",
    "            .join(window_val_means, on=time_window_cols)\n",
    "            .reset_index()\n",
    "        )\n",
    "        windows = window_val_means.reset_index()\n",
    "\n",
    "        X = windows[\"window_index\"]\n",
    "        y = windows[\"value_class\"]\n",
    "        sss = StratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=random_state)\n",
    "        window_idx_train_val, window_idx_test = list(sss.split(X, y))[0]\n",
    "        X_trv = [X[i] for i in sorted(window_idx_train_val)]\n",
    "        X_t = [X[i] for i in sorted(window_idx_test)]\n",
    "        y_trv = [y[i] for i in sorted(window_idx_train_val)]\n",
    "        # y_t = [y[i] for i in sorted(window_idx_test)]\n",
    "\n",
    "        rescaled_frac_val = val_size / (1 - test_size)\n",
    "        sss_trv = StratifiedShuffleSplit(\n",
    "            n_splits=1, test_size=rescaled_frac_val, random_state=random_state + 1\n",
    "        )\n",
    "        window_idx_train, window_idx_val = list(sss_trv.split(X_trv, y_trv))[0]\n",
    "        X_tr = [X_trv[i] for i in sorted(window_idx_train)]\n",
    "        X_v = [X_trv[i] for i in sorted(window_idx_val)]\n",
    "        # y_tr = [y_trv[i] for i in sorted(window_idx_train)]\n",
    "        # y_v = [y_trv[i] for i in sorted(window_idx_val)]\n",
    "\n",
    "        train_inds = np.where(df.window_index.isin(X_tr))[0]\n",
    "        val_inds = np.where(df.window_index.isin(X_v))[0]\n",
    "        test_inds = np.where(df.window_index.isin(X_t))[0]\n",
    "        return {\n",
    "            \"train\": df.iloc[train_inds],\n",
    "            \"val\": df.iloc[val_inds],\n",
    "            \"test\": df.iloc[test_inds],\n",
    "        }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station_name</th>\n",
       "      <th>station_id</th>\n",
       "      <th>imageset_id</th>\n",
       "      <th>image_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>filename</th>\n",
       "      <th>url</th>\n",
       "      <th>flow_cfs</th>\n",
       "      <th>window</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Avery Brook_Bridge_01171000</td>\n",
       "      <td>12</td>\n",
       "      <td>95</td>\n",
       "      <td>153582</td>\n",
       "      <td>2021-03-10 16:01:17+00:00</td>\n",
       "      <td>imagesets/fec63b82-d9fa-4844-ab9b-cda8999122b0...</td>\n",
       "      <td>https://usgs-chs-conte-prod-fpe-storage.s3.ama...</td>\n",
       "      <td>3.39</td>\n",
       "      <td>10</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Avery Brook_Bridge_01171000</td>\n",
       "      <td>12</td>\n",
       "      <td>95</td>\n",
       "      <td>153583</td>\n",
       "      <td>2021-03-10 16:02:24+00:00</td>\n",
       "      <td>imagesets/fec63b82-d9fa-4844-ab9b-cda8999122b0...</td>\n",
       "      <td>https://usgs-chs-conte-prod-fpe-storage.s3.ama...</td>\n",
       "      <td>3.39</td>\n",
       "      <td>10</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  station_name  station_id  imageset_id  image_id  \\\n",
       "0  Avery Brook_Bridge_01171000          12           95    153582   \n",
       "1  Avery Brook_Bridge_01171000          12           95    153583   \n",
       "\n",
       "                  timestamp  \\\n",
       "0 2021-03-10 16:01:17+00:00   \n",
       "1 2021-03-10 16:02:24+00:00   \n",
       "\n",
       "                                            filename  \\\n",
       "0  imagesets/fec63b82-d9fa-4844-ab9b-cda8999122b0...   \n",
       "1  imagesets/fec63b82-d9fa-4844-ab9b-cda8999122b0...   \n",
       "\n",
       "                                                 url  flow_cfs  window  year  \n",
       "0  https://usgs-chs-conte-prod-fpe-storage.s3.ama...      3.39      10  2021  \n",
       "1  https://usgs-chs-conte-prod-fpe-storage.s3.ama...      3.39      10  2021  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitter = WindowedDatasetSplitter(data, \"week\")\n",
    "splitter.data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>window</th>\n",
       "      <th>station_name</th>\n",
       "      <th>station_id</th>\n",
       "      <th>imageset_id</th>\n",
       "      <th>image_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>filename</th>\n",
       "      <th>url</th>\n",
       "      <th>flow_cfs</th>\n",
       "      <th>mean_flow_cfs</th>\n",
       "      <th>val_class</th>\n",
       "      <th>window_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021</td>\n",
       "      <td>10</td>\n",
       "      <td>Avery Brook_Bridge_01171000</td>\n",
       "      <td>12</td>\n",
       "      <td>95</td>\n",
       "      <td>153582</td>\n",
       "      <td>2021-03-10 16:01:17+00:00</td>\n",
       "      <td>imagesets/fec63b82-d9fa-4844-ab9b-cda8999122b0...</td>\n",
       "      <td>https://usgs-chs-conte-prod-fpe-storage.s3.ama...</td>\n",
       "      <td>3.390000</td>\n",
       "      <td>8.019677</td>\n",
       "      <td>med</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021</td>\n",
       "      <td>10</td>\n",
       "      <td>Avery Brook_Bridge_01171000</td>\n",
       "      <td>12</td>\n",
       "      <td>95</td>\n",
       "      <td>153583</td>\n",
       "      <td>2021-03-10 16:02:24+00:00</td>\n",
       "      <td>imagesets/fec63b82-d9fa-4844-ab9b-cda8999122b0...</td>\n",
       "      <td>https://usgs-chs-conte-prod-fpe-storage.s3.ama...</td>\n",
       "      <td>3.390000</td>\n",
       "      <td>8.019677</td>\n",
       "      <td>med</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021</td>\n",
       "      <td>10</td>\n",
       "      <td>Avery Brook_Bridge_01171000</td>\n",
       "      <td>12</td>\n",
       "      <td>95</td>\n",
       "      <td>153584</td>\n",
       "      <td>2021-03-10 16:16:18+00:00</td>\n",
       "      <td>imagesets/fec63b82-d9fa-4844-ab9b-cda8999122b0...</td>\n",
       "      <td>https://usgs-chs-conte-prod-fpe-storage.s3.ama...</td>\n",
       "      <td>3.390000</td>\n",
       "      <td>8.019677</td>\n",
       "      <td>med</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021</td>\n",
       "      <td>10</td>\n",
       "      <td>Avery Brook_Bridge_01171000</td>\n",
       "      <td>12</td>\n",
       "      <td>95</td>\n",
       "      <td>153585</td>\n",
       "      <td>2021-03-10 16:31:18+00:00</td>\n",
       "      <td>imagesets/fec63b82-d9fa-4844-ab9b-cda8999122b0...</td>\n",
       "      <td>https://usgs-chs-conte-prod-fpe-storage.s3.ama...</td>\n",
       "      <td>3.390000</td>\n",
       "      <td>8.019677</td>\n",
       "      <td>med</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021</td>\n",
       "      <td>10</td>\n",
       "      <td>Avery Brook_Bridge_01171000</td>\n",
       "      <td>12</td>\n",
       "      <td>95</td>\n",
       "      <td>153586</td>\n",
       "      <td>2021-03-10 16:46:18+00:00</td>\n",
       "      <td>imagesets/fec63b82-d9fa-4844-ab9b-cda8999122b0...</td>\n",
       "      <td>https://usgs-chs-conte-prod-fpe-storage.s3.ama...</td>\n",
       "      <td>3.390867</td>\n",
       "      <td>8.019677</td>\n",
       "      <td>med</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year  window                 station_name  station_id  imageset_id  \\\n",
       "0  2021      10  Avery Brook_Bridge_01171000          12           95   \n",
       "1  2021      10  Avery Brook_Bridge_01171000          12           95   \n",
       "2  2021      10  Avery Brook_Bridge_01171000          12           95   \n",
       "3  2021      10  Avery Brook_Bridge_01171000          12           95   \n",
       "4  2021      10  Avery Brook_Bridge_01171000          12           95   \n",
       "\n",
       "   image_id                 timestamp  \\\n",
       "0    153582 2021-03-10 16:01:17+00:00   \n",
       "1    153583 2021-03-10 16:02:24+00:00   \n",
       "2    153584 2021-03-10 16:16:18+00:00   \n",
       "3    153585 2021-03-10 16:31:18+00:00   \n",
       "4    153586 2021-03-10 16:46:18+00:00   \n",
       "\n",
       "                                            filename  \\\n",
       "0  imagesets/fec63b82-d9fa-4844-ab9b-cda8999122b0...   \n",
       "1  imagesets/fec63b82-d9fa-4844-ab9b-cda8999122b0...   \n",
       "2  imagesets/fec63b82-d9fa-4844-ab9b-cda8999122b0...   \n",
       "3  imagesets/fec63b82-d9fa-4844-ab9b-cda8999122b0...   \n",
       "4  imagesets/fec63b82-d9fa-4844-ab9b-cda8999122b0...   \n",
       "\n",
       "                                                 url  flow_cfs  mean_flow_cfs  \\\n",
       "0  https://usgs-chs-conte-prod-fpe-storage.s3.ama...  3.390000       8.019677   \n",
       "1  https://usgs-chs-conte-prod-fpe-storage.s3.ama...  3.390000       8.019677   \n",
       "2  https://usgs-chs-conte-prod-fpe-storage.s3.ama...  3.390000       8.019677   \n",
       "3  https://usgs-chs-conte-prod-fpe-storage.s3.ama...  3.390000       8.019677   \n",
       "4  https://usgs-chs-conte-prod-fpe-storage.s3.ama...  3.390867       8.019677   \n",
       "\n",
       "  val_class  window_index  \n",
       "0       med             0  \n",
       "1       med             0  \n",
       "2       med             0  \n",
       "3       med             0  \n",
       "4       med             0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59101\n",
      "59101\n",
      "59101\n"
     ]
    }
   ],
   "source": [
    "randomly_grouped = splitter.shuffle_split(0.7, 0.2, 0.1)\n",
    "stratified_grouped = splitter.stratified_shuffle_split(0.7, 0.2, 0.1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class DatasetSplitter(object):\n",
    "#     \"\"\"Splitters split Datasets into train/validation/test sets.\"\"\"\n",
    "\n",
    "#     def split(self, dataset, frac_train, frac_val, frac_test) -> Dict:\n",
    "#         np.testing.assert_almost_equal(frac_train + frac_val + frac_test, 1.0)\n",
    "#         num_datapoints = len(dataset)\n",
    "#         train_cutoff = int(frac_train * num_datapoints)\n",
    "#         val_cutoff = int((frac_train + frac_val) * num_datapoints)\n",
    "#         indices = np.arange(num_datapoints)\n",
    "#         train_indices = indices[:train_cutoff]\n",
    "#         val_indices = indices[train_cutoff:val_cutoff]\n",
    "#         test_indices = indices[val_cutoff:]\n",
    "#         return {\n",
    "#             \"train\": dataset.iloc[train_indices],\n",
    "#             \"val\": dataset.iloc[val_indices],\n",
    "#             \"test\": dataset.iloc[test_indices],\n",
    "#         }\n",
    "\n",
    "\n",
    "# class RandomStratifiedWindowFlow(DatasetSplitter):\n",
    "#     def split(\n",
    "#         self, dataset, frac_train, frac_val, frac_test, seed=1, window=\"week\"\n",
    "#     ) -> Dict:\n",
    "#         np.testing.assert_almost_equal(frac_train + frac_val + frac_test, 1.0)\n",
    "\n",
    "#         df = dataset.copy()\n",
    "#         df.sort_values(by=\"timestamp\", inplace=True, ignore_index=True)\n",
    "#         if window == \"week\":\n",
    "#             df[\"window\"] = df[\"timestamp\"].dt.isocalendar().week\n",
    "#         elif window == \"day\":\n",
    "#             df[\"window\"] = df[\"timestamp\"].dt.day\n",
    "#         else:\n",
    "#             raise ValueError(\n",
    "#                 \"Window must be 'week' or 'day'. Other windows not yet supported.\"\n",
    "#             )\n",
    "#         df[\"year\"] = df[\"timestamp\"].dt.isocalendar().year\n",
    "\n",
    "#         window_flow_means = (\n",
    "#             df[[\"flow_cfs\", \"year\", \"window\"]]\n",
    "#             .groupby([\"year\", \"window\"])\n",
    "#             .mean()\n",
    "#             .rename(columns={\"flow_cfs\": \"mean_flow_cfs\"})\n",
    "#         )\n",
    "#         # for sites with no flow data, randomly assign a flow value\n",
    "#         if np.all(np.isnan(window_flow_means[\"mean_flow_cfs\"].values)):\n",
    "#             window_flow_means[\"mean_flow_cfs\"] = np.random.uniform(\n",
    "#                 0, 1, len(window_flow_means)\n",
    "#             )\n",
    "#         window_flow_quantiles = np.quantile(\n",
    "#             window_flow_means[\"mean_flow_cfs\"].values, [0.25, 0.75], axis=0\n",
    "#         )\n",
    "#         window_flow_means[\"flow_class\"] = window_flow_means[\"mean_flow_cfs\"].map(\n",
    "#             lambda x: classify3(window_flow_quantiles[0], window_flow_quantiles[1], x)\n",
    "#         )\n",
    "#         window_flow_means[\"window_index\"] = range(len(window_flow_means.index))\n",
    "\n",
    "#         df = (\n",
    "#             df.set_index([\"year\", \"window\"])\n",
    "#             .join(window_flow_means, on=[\"year\", \"window\"])\n",
    "#             .reset_index()\n",
    "#         )\n",
    "#         windows = window_flow_means.reset_index()\n",
    "\n",
    "#         X = windows[\"window_index\"]\n",
    "#         y = windows[\"flow_class\"]\n",
    "#         sss = StratifiedShuffleSplit(n_splits=1, test_size=frac_test, random_state=seed)\n",
    "#         window_idx_train_val, window_idx_test = list(sss.split(X, y))[0]\n",
    "#         X_trv = [X[i] for i in sorted(window_idx_train_val)]\n",
    "#         X_t = [X[i] for i in sorted(window_idx_test)]\n",
    "#         y_trv = [y[i] for i in sorted(window_idx_train_val)]\n",
    "#         # y_t = [y[i] for i in sorted(window_idx_test)]\n",
    "\n",
    "#         rescaled_frac_val = frac_val / (1 - frac_test)\n",
    "#         sss_trv = StratifiedShuffleSplit(\n",
    "#             n_splits=1, test_size=rescaled_frac_val, random_state=seed + 1\n",
    "#         )\n",
    "#         window_idx_train, window_idx_val = list(sss_trv.split(X_trv, y_trv))[0]\n",
    "#         X_tr = [X_trv[i] for i in sorted(window_idx_train)]\n",
    "#         X_v = [X_trv[i] for i in sorted(window_idx_val)]\n",
    "#         # y_tr = [y_trv[i] for i in sorted(window_idx_train)]\n",
    "#         # y_v = [y_trv[i] for i in sorted(window_idx_val)]\n",
    "\n",
    "#         train_inds = np.where(df.window_index.isin(X_tr))[0]\n",
    "#         val_inds = np.where(df.window_index.isin(X_v))[0]\n",
    "#         test_inds = np.where(df.window_index.isin(X_t))[0]\n",
    "#         return {\n",
    "#             \"train\": df.iloc[train_inds],\n",
    "#             \"val\": df.iloc[val_inds],\n",
    "#             \"test\": df.iloc[test_inds],\n",
    "#         }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "streamflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
