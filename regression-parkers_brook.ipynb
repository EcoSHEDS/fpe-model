{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a1b90f3-d954-409d-aa0d-3cacae408259",
   "metadata": {},
   "source": [
    "# Regression Model for Parkers Brook\n",
    "\n",
    "Before running, upload `images.csv` and `images/*.JPG` to S3 bucket (default sagemaker bucket) under prefix: `fpe/data/parkers-brook/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa2bc91d-c209-4b6a-ba13-d7cd92a725f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = \"fpe/parkers-brook\"\n",
    "\n",
    "# role = sagemaker.get_execution_role()\n",
    "role = 'sagemaker-execution-role'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a149ac87-94e6-4d52-a030-4cc4cd5f6e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = \"s3://sagemaker-us-east-1-474916309046/fpe/data/parkers-brook\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b98ea39f-05e0-4079-9793-f134761c14ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "estimator = PyTorch(\n",
    "    entry_point=\"fpe-regression.py\",\n",
    "    role=role,\n",
    "    py_version=\"py38\",\n",
    "    framework_version=\"1.12\",\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.c5.2xlarge\",\n",
    "    hyperparameters={\"epochs\": 6},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c886c9ad-fa43-47c7-86ba-25881b47091b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-10-10 14:24:05 Starting - Starting the training job...\n",
      "2022-10-10 14:24:28 Starting - Preparing the instances for trainingProfilerReport-1665411844: InProgress\n",
      "......\n",
      "2022-10-10 14:25:28 Downloading - Downloading input data.........\n",
      "2022-10-10 14:27:09 Training - Training image download completed. Training in progress.\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2022-10-10 14:26:54,382 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2022-10-10 14:26:54,384 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2022-10-10 14:26:54,390 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2022-10-10 14:26:54,395 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2022-10-10 14:26:54,906 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2022-10-10 14:26:54,915 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2022-10-10 14:26:54,925 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2022-10-10 14:26:54,932 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.c5.2xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"epochs\": 6\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.c5.2xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"job_name\": \"pytorch-training-2022-10-10-14-24-03-856\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-474916309046/pytorch-training-2022-10-10-14-24-03-856/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"fpe-regression\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.c5.2xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.c5.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"fpe-regression.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":6}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=fpe-regression.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.c5.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.c5.2xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.c5.2xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.c5.2xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=fpe-regression\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-474916309046/pytorch-training-2022-10-10-14-24-03-856/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.c5.2xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":6},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.c5.2xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"job_name\":\"pytorch-training-2022-10-10-14-24-03-856\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-474916309046/pytorch-training-2022-10-10-14-24-03-856/source/sourcedir.tar.gz\",\"module_name\":\"fpe-regression\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.c5.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.c5.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"fpe-regression.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"6\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=6\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.22b20221002-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument-3.4.2-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument_cext-0.2.4-py3.8-linux-x86_64.egg\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 fpe-regression.py --epochs 6\u001b[0m\n",
      "\u001b[34mtorch: 1.12.1+cpu\u001b[0m\n",
      "\u001b[34mtorchvision: 0.13.1+cpu\u001b[0m\n",
      "\u001b[34margs:\n",
      "  batch_size: 64\n",
      "  test_batch_size: 64\n",
      "  epochs: 6\n",
      "  lr: 0.001\n",
      "  momentum: 0.5\n",
      "  seed: 1\n",
      "  log_interval: 100\n",
      "  backend: None\n",
      "  hosts: ['algo-1']\n",
      "  current_host: algo-1\n",
      "  model_dir: /opt/ml/model\n",
      "  data_dir: /opt/ml/input/data/training\n",
      "  num_gpus: 0\n",
      "  filename: images.csv\n",
      "  head: None\n",
      "  test_size: 0.2\n",
      "  unfreeze_after: 2\u001b[0m\n",
      "\u001b[34mload dataset: /opt/ml/input/data/training/images.csv\u001b[0m\n",
      "\u001b[34mfilter(hour): 7 to 18\u001b[0m\n",
      "\u001b[34mfilter(month): 4 to 11\u001b[0m\n",
      "\u001b[34mdataset loaded\n",
      "  rows: 2867\n",
      "  flow: 11.74 cfs\u001b[0m\n",
      "\u001b[34mdataset split\u001b[0m\n",
      "\u001b[34mtrain\n",
      "    rows: 2299\n",
      "    flow: 2.59, 11.47, 93.76 cfs\u001b[0m\n",
      "\u001b[34mtest\n",
      "    rows: 568\n",
      "    flow: 2.84, 12.85, 61.46 cfs\u001b[0m\n",
      "\u001b[34mloading first image: /opt/ml/input/data/training/images/20180613_1230_ParkersBrook.JPG\u001b[0m\n",
      "\u001b[34mshape: (3, 384, 682)\u001b[0m\n",
      "\u001b[34mDownloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\u001b[0m\n",
      "\u001b[34m0%|          | 0.00/44.7M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34m64%|██████▎   | 28.4M/44.7M [00:00<00:00, 298MB/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 44.7M/44.7M [00:00<00:00, 291MB/s]\u001b[0m\n",
      "\u001b[34mtrain: start\u001b[0m\n",
      "\u001b[34mepoch [ 1/ 6]\u001b[0m\n",
      "\u001b[34m[2022-10-10 14:27:00.416 algo-1:36 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2022-10-10 14:27:00.672 algo-1:36 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2022-10-10 14:27:00.673 algo-1:36 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2022-10-10 14:27:00.673 algo-1:36 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2022-10-10 14:27:00.674 algo-1:36 INFO hook.py:254] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2022-10-10 14:27:00.674 algo-1:36 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34mtrain [batch     1/   36] loss: 5.401515\u001b[0m\n",
      "\u001b[34mtrain [batch     2/   36] loss: 5.004740\u001b[0m\n",
      "\u001b[34mtrain [batch     3/   36] loss: 5.058692\u001b[0m\n",
      "\u001b[34mtrain [batch     4/   36] loss: 4.872710\u001b[0m\n",
      "\u001b[34mtrain [batch     5/   36] loss: 4.669394\u001b[0m\n",
      "\u001b[34mtrain [batch     6/   36] loss: 4.522284\u001b[0m\n",
      "\u001b[34mtrain [batch     7/   36] loss: 4.071532\u001b[0m\n",
      "\u001b[34mtrain [batch     8/   36] loss: 3.693892\u001b[0m\n",
      "\u001b[34mtrain [batch     9/   36] loss: 3.462747\u001b[0m\n",
      "\u001b[34mtrain [batch    10/   36] loss: 3.077455\u001b[0m\n",
      "\u001b[34mtrain [batch    11/   36] loss: 2.567915\u001b[0m\n",
      "\u001b[34mtrain [batch    12/   36] loss: 2.112859\u001b[0m\n",
      "\u001b[34mtrain [batch    13/   36] loss: 2.078678\u001b[0m\n",
      "\u001b[34mtrain [batch    14/   36] loss: 1.191328\u001b[0m\n",
      "\u001b[34mtrain [batch    15/   36] loss: 0.905037\u001b[0m\n",
      "\u001b[34mtrain [batch    16/   36] loss: 1.252609\u001b[0m\n",
      "\u001b[34mtrain [batch    17/   36] loss: 0.832284\u001b[0m\n",
      "\u001b[34mtrain [batch    18/   36] loss: 0.737827\u001b[0m\n",
      "\u001b[34mtrain [batch    19/   36] loss: 0.646343\u001b[0m\n",
      "\u001b[34mtrain [batch    20/   36] loss: 0.514795\u001b[0m\n",
      "\u001b[34mtrain [batch    21/   36] loss: 0.550166\u001b[0m\n",
      "\u001b[34mtrain [batch    22/   36] loss: 0.560958\u001b[0m\n",
      "\u001b[34mtrain [batch    23/   36] loss: 0.419704\u001b[0m\n",
      "\u001b[34mtrain [batch    24/   36] loss: 0.527700\u001b[0m\n",
      "\u001b[34mtrain [batch    25/   36] loss: 0.454502\u001b[0m\n",
      "\u001b[34mtrain [batch    26/   36] loss: 0.385524\u001b[0m\n",
      "\u001b[34mtrain [batch    27/   36] loss: 0.704894\u001b[0m\n",
      "\u001b[34mtrain [batch    28/   36] loss: 0.383584\u001b[0m\n",
      "\u001b[34mtrain [batch    29/   36] loss: 0.597176\u001b[0m\n",
      "\u001b[34mtrain [batch    30/   36] loss: 0.548808\u001b[0m\n",
      "\u001b[34mtrain [batch    31/   36] loss: 0.537756\u001b[0m\n",
      "\u001b[34mtrain [batch    32/   36] loss: 0.553713\u001b[0m\n",
      "\u001b[34mtrain [batch    33/   36] loss: 0.442236\u001b[0m\n",
      "\u001b[34mtrain [batch    34/   36] loss: 0.675563\u001b[0m\n",
      "\u001b[34mtrain [batch    35/   36] loss: 0.668645\u001b[0m\n",
      "\u001b[34mtrain [batch    36/   36] loss: 0.481012\u001b[0m\n",
      "\u001b[34mtrain [average] loss: 1.810183\u001b[0m\n",
      "\u001b[34mtest [batch     1/    9] loss: 0.072889\u001b[0m\n",
      "\u001b[34mtest [batch     2/    9] loss: 0.038148\u001b[0m\n",
      "\u001b[34mtest [batch     3/    9] loss: 0.935805\u001b[0m\n",
      "\u001b[34mtest [batch     4/    9] loss: 0.447186\u001b[0m\n",
      "\u001b[34mtest [batch     5/    9] loss: 0.507615\u001b[0m\n",
      "\u001b[34mtest [batch     6/    9] loss: 1.040270\u001b[0m\n",
      "\u001b[34mtest [batch     7/    9] loss: 1.343939\u001b[0m\n",
      "\u001b[34mtest [batch     8/    9] loss: 0.383362\u001b[0m\n",
      "\u001b[34mtest [batch     9/    9] loss: 0.598343\u001b[0m\n",
      "\u001b[34mtest [average] loss): 0.596395\u001b[0m\n",
      "\u001b[34mepoch [ 2/ 6]\u001b[0m\n",
      "\u001b[34mtrain [batch     1/   36] loss: 0.401684\u001b[0m\n",
      "\u001b[34mtrain [batch     2/   36] loss: 0.458292\u001b[0m\n",
      "\u001b[34mtrain [batch     3/   36] loss: 0.505616\u001b[0m\n",
      "\u001b[34mtrain [batch     4/   36] loss: 0.591090\u001b[0m\n",
      "\u001b[34mtrain [batch     5/   36] loss: 0.610989\u001b[0m\n",
      "\u001b[34mtrain [batch     6/   36] loss: 0.728263\u001b[0m\n",
      "\u001b[34mtrain [batch     7/   36] loss: 0.396826\u001b[0m\n",
      "\u001b[34mtrain [batch     8/   36] loss: 0.417726\u001b[0m\n",
      "\u001b[34mtrain [batch     9/   36] loss: 0.505404\u001b[0m\n",
      "\u001b[34mtrain [batch    10/   36] loss: 0.420835\u001b[0m\n",
      "\u001b[34mtrain [batch    11/   36] loss: 0.388866\u001b[0m\n",
      "\u001b[34mtrain [batch    12/   36] loss: 0.417680\u001b[0m\n",
      "\u001b[34mtrain [batch    13/   36] loss: 0.594776\u001b[0m\n",
      "\u001b[34mtrain [batch    14/   36] loss: 0.423324\u001b[0m\n",
      "\u001b[34mtrain [batch    15/   36] loss: 0.614424\u001b[0m\n",
      "\u001b[34mtrain [batch    16/   36] loss: 0.456052\u001b[0m\n",
      "\u001b[34mtrain [batch    17/   36] loss: 0.443925\u001b[0m\n",
      "\u001b[34mtrain [batch    18/   36] loss: 0.528876\u001b[0m\n",
      "\u001b[34mtrain [batch    19/   36] loss: 0.436167\u001b[0m\n",
      "\u001b[34mtrain [batch    20/   36] loss: 0.526516\u001b[0m\n",
      "\u001b[34mtrain [batch    21/   36] loss: 0.435838\u001b[0m\n",
      "\u001b[34mtrain [batch    22/   36] loss: 0.459139\u001b[0m\n",
      "\u001b[34mtrain [batch    23/   36] loss: 0.552739\u001b[0m\n",
      "\u001b[34mtrain [batch    24/   36] loss: 0.422332\u001b[0m\n",
      "\u001b[34mtrain [batch    25/   36] loss: 0.546894\u001b[0m\n",
      "\u001b[34mtrain [batch    26/   36] loss: 0.475137\u001b[0m\n",
      "\u001b[34mtrain [batch    27/   36] loss: 0.403505\u001b[0m\n",
      "\u001b[34mtrain [batch    28/   36] loss: 0.588494\u001b[0m\n",
      "\u001b[34mtrain [batch    29/   36] loss: 0.581667\u001b[0m\n",
      "\u001b[34mtrain [batch    30/   36] loss: 0.537902\u001b[0m\n",
      "\u001b[34mtrain [batch    31/   36] loss: 0.416624\u001b[0m\n",
      "\u001b[34mtrain [batch    32/   36] loss: 0.432402\u001b[0m\n",
      "\u001b[34mtrain [batch    33/   36] loss: 0.378477\u001b[0m\n",
      "\u001b[34mtrain [batch    34/   36] loss: 0.472092\u001b[0m\n",
      "\u001b[34mtrain [batch    35/   36] loss: 0.565222\u001b[0m\n",
      "\u001b[34mtrain [batch    36/   36] loss: 0.612807\u001b[0m\n",
      "\u001b[34mtrain [average] loss: 0.493017\u001b[0m\n",
      "\u001b[34mtest [batch     1/    9] loss: 0.073523\u001b[0m\n",
      "\u001b[34mtest [batch     2/    9] loss: 0.035646\u001b[0m\n",
      "\u001b[34mtest [batch     3/    9] loss: 0.891434\u001b[0m\n",
      "\u001b[34mtest [batch     4/    9] loss: 0.394394\u001b[0m\n",
      "\u001b[34mtest [batch     5/    9] loss: 0.436553\u001b[0m\n",
      "\u001b[34mtest [batch     6/    9] loss: 0.842784\u001b[0m\n",
      "\u001b[34mtest [batch     7/    9] loss: 1.214786\u001b[0m\n",
      "\u001b[34mtest [batch     8/    9] loss: 0.342717\u001b[0m\n",
      "\u001b[34mtest [batch     9/    9] loss: 0.521851\u001b[0m\n",
      "\u001b[34mtest [average] loss): 0.528188\u001b[0m\n",
      "\u001b[34mmodel:  unfreeze resnet\u001b[0m\n",
      "\u001b[34mepoch [ 3/ 6]\u001b[0m\n",
      "\u001b[34mtrain [batch     1/   36] loss: 0.487502\u001b[0m\n",
      "\u001b[34mtrain [batch     2/   36] loss: 0.477964\u001b[0m\n",
      "\u001b[34mtrain [batch     3/   36] loss: 0.394616\u001b[0m\n",
      "\u001b[34mtrain [batch     4/   36] loss: 0.363620\u001b[0m\n",
      "\u001b[34mtrain [batch     5/   36] loss: 0.310122\u001b[0m\n",
      "\u001b[34mtrain [batch     6/   36] loss: 0.295293\u001b[0m\n",
      "\u001b[34mtrain [batch     7/   36] loss: 0.313339\u001b[0m\n",
      "\u001b[34mtrain [batch     8/   36] loss: 0.293191\u001b[0m\n",
      "\u001b[34mtrain [batch     9/   36] loss: 0.275980\u001b[0m\n",
      "\u001b[34mtrain [batch    10/   36] loss: 0.324089\u001b[0m\n",
      "\u001b[34mtrain [batch    11/   36] loss: 0.244125\u001b[0m\n",
      "\u001b[34mtrain [batch    12/   36] loss: 0.279202\u001b[0m\n",
      "\u001b[34mtrain [batch    13/   36] loss: 0.241467\u001b[0m\n",
      "\u001b[34mtrain [batch    14/   36] loss: 0.411596\u001b[0m\n",
      "\u001b[34mtrain [batch    15/   36] loss: 0.387882\u001b[0m\n",
      "\u001b[34mtrain [batch    16/   36] loss: 0.313317\u001b[0m\n",
      "\u001b[34mtrain [batch    17/   36] loss: 0.241669\u001b[0m\n",
      "\u001b[34mtrain [batch    18/   36] loss: 0.203984\u001b[0m\n",
      "\u001b[34mtrain [batch    19/   36] loss: 0.316469\u001b[0m\n",
      "\u001b[34mtrain [batch    20/   36] loss: 0.174921\u001b[0m\n",
      "\u001b[34mtrain [batch    21/   36] loss: 0.121527\u001b[0m\n",
      "\u001b[34mtrain [batch    22/   36] loss: 0.332307\u001b[0m\n",
      "\u001b[34mtrain [batch    23/   36] loss: 0.355444\u001b[0m\n",
      "\u001b[34mtrain [batch    24/   36] loss: 0.290036\u001b[0m\n",
      "\u001b[34mtrain [batch    25/   36] loss: 0.214255\u001b[0m\n",
      "\u001b[34mtrain [batch    26/   36] loss: 0.174463\u001b[0m\n",
      "\u001b[34mtrain [batch    27/   36] loss: 0.155879\u001b[0m\n",
      "\u001b[34mtrain [batch    28/   36] loss: 0.158422\u001b[0m\n",
      "\u001b[34mtrain [batch    29/   36] loss: 0.263146\u001b[0m\n",
      "\u001b[34mtrain [batch    30/   36] loss: 0.445313\u001b[0m\n",
      "\u001b[34mtrain [batch    31/   36] loss: 0.286605\u001b[0m\n",
      "\u001b[34mtrain [batch    32/   36] loss: 0.336955\u001b[0m\n",
      "\u001b[34mtrain [batch    33/   36] loss: 0.258416\u001b[0m\n",
      "\u001b[34mtrain [batch    34/   36] loss: 0.186892\u001b[0m\n",
      "\u001b[34mtrain [batch    35/   36] loss: 0.176658\u001b[0m\n",
      "\u001b[34mtrain [batch    36/   36] loss: 0.267658\u001b[0m\n",
      "\u001b[34mtrain [average] loss: 0.288176\u001b[0m\n",
      "\u001b[34mtest [batch     1/    9] loss: 0.051460\u001b[0m\n",
      "\u001b[34mtest [batch     2/    9] loss: 0.023441\u001b[0m\n",
      "\u001b[34mtest [batch     3/    9] loss: 0.500456\u001b[0m\n",
      "\u001b[34mtest [batch     4/    9] loss: 0.134031\u001b[0m\n",
      "\u001b[34mtest [batch     5/    9] loss: 0.098826\u001b[0m\n",
      "\u001b[34mtest [batch     6/    9] loss: 0.082868\u001b[0m\n",
      "\u001b[34mtest [batch     7/    9] loss: 0.185225\u001b[0m\n",
      "\u001b[34mtest [batch     8/    9] loss: 0.172441\u001b[0m\n",
      "\u001b[34mtest [batch     9/    9] loss: 0.205608\u001b[0m\n",
      "\u001b[34mtest [average] loss): 0.161595\u001b[0m\n",
      "\u001b[34mepoch [ 4/ 6]\u001b[0m\n",
      "\u001b[34mtrain [batch     1/   36] loss: 0.144102\u001b[0m\n",
      "\u001b[34mtrain [batch     2/   36] loss: 0.117612\u001b[0m\n",
      "\u001b[34mtrain [batch     3/   36] loss: 0.147031\u001b[0m\n",
      "\u001b[34mtrain [batch     4/   36] loss: 0.167869\u001b[0m\n",
      "\u001b[34mtrain [batch     5/   36] loss: 0.123935\u001b[0m\n",
      "\u001b[34mtrain [batch     6/   36] loss: 0.141182\u001b[0m\n",
      "\u001b[34mtrain [batch     7/   36] loss: 0.102246\u001b[0m\n",
      "\u001b[34mtrain [batch     8/   36] loss: 0.148825\u001b[0m\n",
      "\u001b[34mtrain [batch     9/   36] loss: 0.119484\u001b[0m\n",
      "\u001b[34mtrain [batch    10/   36] loss: 0.152616\u001b[0m\n",
      "\u001b[34mtrain [batch    11/   36] loss: 0.178444\u001b[0m\n",
      "\u001b[34mtrain [batch    12/   36] loss: 0.132066\u001b[0m\n",
      "\u001b[34mtrain [batch    13/   36] loss: 0.086851\u001b[0m\n",
      "\u001b[34mtrain [batch    14/   36] loss: 0.112973\u001b[0m\n",
      "\u001b[34mtrain [batch    15/   36] loss: 0.090865\u001b[0m\n",
      "\u001b[34mtrain [batch    16/   36] loss: 0.124813\u001b[0m\n",
      "\u001b[34mtrain [batch    17/   36] loss: 0.072327\u001b[0m\n",
      "\u001b[34mtrain [batch    18/   36] loss: 0.095906\u001b[0m\n",
      "\u001b[34mtrain [batch    19/   36] loss: 0.125703\u001b[0m\n",
      "\u001b[34mtrain [batch    20/   36] loss: 0.091351\u001b[0m\n",
      "\u001b[34mtrain [batch    21/   36] loss: 0.113431\u001b[0m\n",
      "\u001b[34mtrain [batch    22/   36] loss: 0.114115\u001b[0m\n",
      "\u001b[34mtrain [batch    23/   36] loss: 0.150198\u001b[0m\n",
      "\u001b[34mtrain [batch    24/   36] loss: 0.096866\u001b[0m\n",
      "\u001b[34mtrain [batch    25/   36] loss: 0.054091\u001b[0m\n",
      "\u001b[34mtrain [batch    26/   36] loss: 0.088311\u001b[0m\n",
      "\u001b[34mtrain [batch    27/   36] loss: 0.092560\u001b[0m\n",
      "\u001b[34mtrain [batch    28/   36] loss: 0.112027\u001b[0m\n",
      "\u001b[34mtrain [batch    29/   36] loss: 0.121543\u001b[0m\n",
      "\u001b[34mtrain [batch    30/   36] loss: 0.132170\u001b[0m\n",
      "\u001b[34mtrain [batch    31/   36] loss: 0.062010\u001b[0m\n",
      "\u001b[34mtrain [batch    32/   36] loss: 0.134807\u001b[0m\n",
      "\u001b[34mtrain [batch    33/   36] loss: 0.118521\u001b[0m\n",
      "\u001b[34mtrain [batch    34/   36] loss: 0.142534\u001b[0m\n",
      "\u001b[34mtrain [batch    35/   36] loss: 0.275784\u001b[0m\n",
      "\u001b[34mtrain [batch    36/   36] loss: 0.414726\u001b[0m\n",
      "\u001b[34mtrain [average] loss: 0.130553\u001b[0m\n",
      "\u001b[34mtest [batch     1/    9] loss: 0.045850\u001b[0m\n",
      "\u001b[34mtest [batch     2/    9] loss: 0.026983\u001b[0m\n",
      "\u001b[34mtest [batch     3/    9] loss: 0.396365\u001b[0m\n",
      "\u001b[34mtest [batch     4/    9] loss: 0.111465\u001b[0m\n",
      "\u001b[34mtest [batch     5/    9] loss: 0.075151\u001b[0m\n",
      "\u001b[34mtest [batch     6/    9] loss: 0.068848\u001b[0m\n",
      "\u001b[34mtest [batch     7/    9] loss: 0.100932\u001b[0m\n",
      "\u001b[34mtest [batch     8/    9] loss: 0.164707\u001b[0m\n",
      "\u001b[34mtest [batch     9/    9] loss: 0.212161\u001b[0m\n",
      "\u001b[34mtest [average] loss): 0.133607\u001b[0m\n",
      "\u001b[34mepoch [ 5/ 6]\u001b[0m\n",
      "\u001b[34mtrain [batch     1/   36] loss: 0.194672\u001b[0m\n",
      "\u001b[34mtrain [batch     2/   36] loss: 0.118628\u001b[0m\n",
      "\u001b[34mtrain [batch     3/   36] loss: 0.090499\u001b[0m\n",
      "\u001b[34mtrain [batch     4/   36] loss: 0.096138\u001b[0m\n",
      "\u001b[34mtrain [batch     5/   36] loss: 0.132699\u001b[0m\n",
      "\u001b[34mtrain [batch     6/   36] loss: 0.058500\u001b[0m\n",
      "\u001b[34mtrain [batch     7/   36] loss: 0.104692\u001b[0m\n",
      "\u001b[34mtrain [batch     8/   36] loss: 0.118532\u001b[0m\n",
      "\u001b[34mtrain [batch     9/   36] loss: 0.154741\u001b[0m\n",
      "\u001b[34mtrain [batch    10/   36] loss: 0.127182\u001b[0m\n",
      "\u001b[34mtrain [batch    11/   36] loss: 0.088966\u001b[0m\n",
      "\u001b[34mtrain [batch    12/   36] loss: 0.069933\u001b[0m\n",
      "\u001b[34mtrain [batch    13/   36] loss: 0.106886\u001b[0m\n",
      "\u001b[34mtrain [batch    14/   36] loss: 0.107017\u001b[0m\n",
      "\u001b[34mtrain [batch    15/   36] loss: 0.058385\u001b[0m\n",
      "\u001b[34mtrain [batch    16/   36] loss: 0.109937\u001b[0m\n",
      "\u001b[34mtrain [batch    17/   36] loss: 0.081636\u001b[0m\n",
      "\u001b[34mtrain [batch    18/   36] loss: 0.111794\u001b[0m\n",
      "\u001b[34mtrain [batch    19/   36] loss: 0.169829\u001b[0m\n",
      "\u001b[34mtrain [batch    20/   36] loss: 0.135322\u001b[0m\n",
      "\u001b[34mtrain [batch    21/   36] loss: 0.075378\u001b[0m\n",
      "\u001b[34mtrain [batch    22/   36] loss: 0.135623\u001b[0m\n",
      "\u001b[34mtrain [batch    23/   36] loss: 0.178084\u001b[0m\n",
      "\u001b[34mtrain [batch    24/   36] loss: 0.140836\u001b[0m\n",
      "\u001b[34mtrain [batch    25/   36] loss: 0.074165\u001b[0m\n",
      "\u001b[34mtrain [batch    26/   36] loss: 0.155248\u001b[0m\n",
      "\u001b[34mtrain [batch    27/   36] loss: 0.123116\u001b[0m\n",
      "\u001b[34mtrain [batch    28/   36] loss: 0.072113\u001b[0m\n",
      "\u001b[34mtrain [batch    29/   36] loss: 0.075571\u001b[0m\n",
      "\u001b[34mtrain [batch    30/   36] loss: 0.084012\u001b[0m\n",
      "\u001b[34mtrain [batch    31/   36] loss: 0.090991\u001b[0m\n",
      "\u001b[34mtrain [batch    32/   36] loss: 0.073785\u001b[0m\n",
      "\u001b[34mtrain [batch    33/   36] loss: 0.075504\u001b[0m\n",
      "\u001b[34mtrain [batch    34/   36] loss: 0.095921\u001b[0m\n",
      "\u001b[34mtrain [batch    35/   36] loss: 0.098302\u001b[0m\n",
      "\u001b[34mtrain [batch    36/   36] loss: 0.123454\u001b[0m\n",
      "\u001b[34mtrain [average] loss: 0.108558\u001b[0m\n",
      "\u001b[34mtest [batch     1/    9] loss: 0.100961\u001b[0m\n",
      "\u001b[34mtest [batch     2/    9] loss: 0.020502\u001b[0m\n",
      "\u001b[34mtest [batch     3/    9] loss: 0.153755\u001b[0m\n",
      "\u001b[34mtest [batch     4/    9] loss: 0.087087\u001b[0m\n",
      "\u001b[34mtest [batch     5/    9] loss: 0.059046\u001b[0m\n",
      "\u001b[34mtest [batch     6/    9] loss: 0.052832\u001b[0m\n",
      "\u001b[34mtest [batch     7/    9] loss: 0.098224\u001b[0m\n",
      "\u001b[34mtest [batch     8/    9] loss: 0.098048\u001b[0m\n",
      "\u001b[34mtest [batch     9/    9] loss: 0.087317\u001b[0m\n",
      "\u001b[34mtest [average] loss): 0.084197\u001b[0m\n",
      "\u001b[34mepoch [ 6/ 6]\u001b[0m\n",
      "\u001b[34mtrain [batch     1/   36] loss: 0.055536\u001b[0m\n",
      "\u001b[34mtrain [batch     2/   36] loss: 0.094105\u001b[0m\n",
      "\u001b[34mtrain [batch     3/   36] loss: 0.074676\u001b[0m\n",
      "\u001b[34mtrain [batch     4/   36] loss: 0.096330\u001b[0m\n",
      "\u001b[34mtrain [batch     5/   36] loss: 0.064748\u001b[0m\n",
      "\u001b[34mtrain [batch     6/   36] loss: 0.059343\u001b[0m\n",
      "\u001b[34mtrain [batch     7/   36] loss: 0.070967\u001b[0m\n",
      "\u001b[34mtrain [batch     8/   36] loss: 0.059686\u001b[0m\n",
      "\u001b[34mtrain [batch     9/   36] loss: 0.076659\u001b[0m\n",
      "\u001b[34mtrain [batch    10/   36] loss: 0.087928\u001b[0m\n",
      "\u001b[34mtrain [batch    11/   36] loss: 0.082786\u001b[0m\n",
      "\u001b[34mtrain [batch    12/   36] loss: 0.062849\u001b[0m\n",
      "\u001b[34mtrain [batch    13/   36] loss: 0.081786\u001b[0m\n",
      "\u001b[34mtrain [batch    14/   36] loss: 0.065282\u001b[0m\n",
      "\u001b[34mtrain [batch    15/   36] loss: 0.078382\u001b[0m\n",
      "\u001b[34mtrain [batch    16/   36] loss: 0.050434\u001b[0m\n",
      "\u001b[34mtrain [batch    17/   36] loss: 0.083572\u001b[0m\n",
      "\u001b[34mtrain [batch    18/   36] loss: 0.094852\u001b[0m\n",
      "\u001b[34mtrain [batch    19/   36] loss: 0.056819\u001b[0m\n",
      "\u001b[34mtrain [batch    20/   36] loss: 0.113526\u001b[0m\n",
      "\u001b[34mtrain [batch    21/   36] loss: 0.069853\u001b[0m\n",
      "\u001b[34mtrain [batch    22/   36] loss: 0.074214\u001b[0m\n",
      "\u001b[34mtrain [batch    23/   36] loss: 0.038654\u001b[0m\n",
      "\u001b[34mtrain [batch    24/   36] loss: 0.036547\u001b[0m\n",
      "\u001b[34mtrain [batch    25/   36] loss: 0.088879\u001b[0m\n",
      "\u001b[34mtrain [batch    26/   36] loss: 0.066349\u001b[0m\n",
      "\u001b[34mtrain [batch    27/   36] loss: 0.062465\u001b[0m\n",
      "\u001b[34mtrain [batch    28/   36] loss: 0.092465\u001b[0m\n",
      "\u001b[34mtrain [batch    29/   36] loss: 0.079256\u001b[0m\n",
      "\u001b[34mtrain [batch    30/   36] loss: 0.139930\u001b[0m\n",
      "\u001b[34mtrain [batch    31/   36] loss: 0.213450\u001b[0m\n",
      "\u001b[34mtrain [batch    32/   36] loss: 0.116125\u001b[0m\n",
      "\u001b[34mtrain [batch    33/   36] loss: 0.134993\u001b[0m\n",
      "\u001b[34mtrain [batch    34/   36] loss: 0.085556\u001b[0m\n",
      "\u001b[34mtrain [batch    35/   36] loss: 0.063867\u001b[0m\n",
      "\u001b[34mtrain [batch    36/   36] loss: 0.077791\u001b[0m\n",
      "\u001b[34mtrain [average] loss: 0.081963\u001b[0m\n",
      "\u001b[34mtest [batch     1/    9] loss: 0.179387\u001b[0m\n",
      "\u001b[34mtest [batch     2/    9] loss: 0.034975\u001b[0m\n",
      "\u001b[34mtest [batch     3/    9] loss: 0.072831\u001b[0m\n",
      "\u001b[34mtest [batch     4/    9] loss: 0.089976\u001b[0m\n",
      "\u001b[34mtest [batch     5/    9] loss: 0.064328\u001b[0m\n",
      "\u001b[34mtest [batch     6/    9] loss: 0.049596\u001b[0m\n",
      "\u001b[34mtest [batch     7/    9] loss: 0.102847\u001b[0m\n",
      "\u001b[34mtest [batch     8/    9] loss: 0.077504\u001b[0m\n",
      "\u001b[34mtest [batch     9/    9] loss: 0.034325\u001b[0m\n",
      "\u001b[34mtest [average] loss): 0.078419\u001b[0m\n",
      "\u001b[34msave model: /opt/ml/model/model.pth\u001b[0m\n",
      "\u001b[34mINFO:__main__:save model: /opt/ml/model/model.pth\u001b[0m\n",
      "\u001b[34mtrain: end\u001b[0m\n",
      "\u001b[34minference: start\u001b[0m\n",
      "\u001b[34mINFO:__main__:train: end\u001b[0m\n",
      "\u001b[34mINFO:__main__:inference: start\u001b[0m\n",
      "\u001b[34mbatch [    1/   45]\u001b[0m\n",
      "\u001b[34mDEBUG:__main__:  batch [    1/   45]\u001b[0m\n",
      "\u001b[34mbatch [    2/   45]\u001b[0m\n",
      "\u001b[34mDEBUG:__main__:  batch [    2/   45]\u001b[0m\n",
      "\u001b[34mbatch [    3/   45]\u001b[0m\n",
      "\u001b[34mDEBUG:__main__:  batch [    3/   45]\u001b[0m\n",
      "\u001b[34mbatch [    4/   45]\u001b[0m\n",
      "\u001b[34mDEBUG:__main__:  batch [    4/   45]\u001b[0m\n",
      "\u001b[34mbatch [    5/   45]\u001b[0m\n",
      "\u001b[34mDEBUG:__main__:  batch [    5/   45]\u001b[0m\n",
      "\u001b[34mbatch [    6/   45]\u001b[0m\n",
      "\u001b[34mDEBUG:__main__:  batch [    6/   45]\u001b[0m\n",
      "\u001b[34mbatch [    7/   45]\u001b[0m\n",
      "\u001b[34mDEBUG:__main__:  batch [    7/   45]\u001b[0m\n",
      "\u001b[34mbatch [    8/   45]\u001b[0m\n",
      "\u001b[34mDEBUG:__main__:  batch [    8/   45]\u001b[0m\n",
      "\u001b[34mbatch [    9/   45]\u001b[0m\n",
      "\u001b[34mDEBUG:__main__:  batch [    9/   45]\u001b[0m\n",
      "\u001b[34mbatch [   10/   45]\u001b[0m\n",
      "\u001b[34mDEBUG:__main__:  batch [   10/   45]\u001b[0m\n",
      "\u001b[34mbatch [   11/   45]\u001b[0m\n",
      "\u001b[34mDEBUG:__main__:  batch [   11/   45]\u001b[0m\n",
      "\u001b[34mbatch [   12/   45]\u001b[0m\n",
      "\u001b[34mDEBUG:__main__:  batch [   12/   45]\u001b[0m\n",
      "\u001b[34mbatch [   13/   45]\u001b[0m\n",
      "\u001b[34mDEBUG:__main__:  batch [   13/   45]\u001b[0m\n",
      "\u001b[34mbatch [   14/   45]\u001b[0m\n",
      "\u001b[34mDEBUG:__main__:  batch [   14/   45]\u001b[0m\n",
      "\u001b[34mbatch [   15/   45]\u001b[0m\n",
      "\u001b[34mDEBUG:__main__:  batch [   15/   45]\u001b[0m\n",
      "\u001b[34mbatch [   16/   45]\u001b[0m\n",
      "\u001b[34mDEBUG:__main__:  batch [   16/   45]\u001b[0m\n",
      "\u001b[34mbatch [   17/   45]\u001b[0m\n",
      "\u001b[34mDEBUG:__main__:  batch [   17/   45]\u001b[0m\n",
      "\u001b[34mbatch [   18/   45]\u001b[0m\n",
      "\u001b[34mDEBUG:__main__:  batch [   18/   45]\u001b[0m\n",
      "\u001b[34mbatch [   19/   45]\u001b[0m\n",
      "\u001b[34mDEBUG:__main__:  batch [   19/   45]\u001b[0m\n",
      "\u001b[34mbatch [   20/   45]\u001b[0m\n",
      "\u001b[34mDEBUG:__main__:  batch [   20/   45]\u001b[0m\n",
      "\u001b[34mbatch [   21/   45]\u001b[0m\n",
      "\u001b[34mDEBUG:__main__:  batch [   21/   45]\u001b[0m\n",
      "\u001b[34mbatch [   22/   45]\u001b[0m\n",
      "\u001b[34mDEBUG:__main__:  batch [   22/   45]\u001b[0m\n",
      "\u001b[34mbatch [   23/   45]\u001b[0m\n",
      "\u001b[34mDEBUG:__main__:  batch [   23/   45]\u001b[0m\n",
      "\u001b[34mbatch [   24/   45]\u001b[0m\n",
      "\u001b[34mDEBUG:__main__:  batch [   24/   45]\u001b[0m\n",
      "\u001b[34mbatch [   25/   45]\u001b[0m\n",
      "\u001b[34mDEBUG:__main__:  batch [   25/   45]\u001b[0m\n",
      "\u001b[34mbatch [   26/   45]\u001b[0m\n",
      "\u001b[34mDEBUG:__main__:  batch [   26/   45]\u001b[0m\n",
      "\u001b[34mbatch [   27/   45]\u001b[0m\n",
      "\u001b[34mDEBUG:__main__:  batch [   27/   45]\u001b[0m\n",
      "\u001b[34mbatch [   28/   45]\u001b[0m\n",
      "\u001b[34mDEBUG:__main__:  batch [   28/   45]\u001b[0m\n",
      "\u001b[34mbatch [   29/   45]\u001b[0m\n",
      "\u001b[34mDEBUG:__main__:  batch [   29/   45]\u001b[0m\n",
      "\u001b[34mbatch [   30/   45]\u001b[0m\n",
      "\u001b[34mDEBUG:__main__:  batch [   30/   45]\u001b[0m\n",
      "\u001b[34mbatch [   31/   45]\u001b[0m\n",
      "\u001b[34mDEBUG:__main__:  batch [   31/   45]\u001b[0m\n",
      "\u001b[34mbatch [   32/   45]\u001b[0m\n",
      "\u001b[34mDEBUG:__main__:  batch [   32/   45]\u001b[0m\n",
      "\u001b[34mbatch [   33/   45]\u001b[0m\n",
      "\u001b[34mDEBUG:__main__:  batch [   33/   45]\u001b[0m\n",
      "\u001b[34mbatch [   34/   45]\u001b[0m\n",
      "\u001b[34mDEBUG:__main__:  batch [   34/   45]\u001b[0m\n",
      "\u001b[34mbatch [   35/   45]\u001b[0m\n",
      "\u001b[34mDEBUG:__main__:  batch [   35/   45]\u001b[0m\n",
      "\u001b[34mbatch [   36/   45]\u001b[0m\n",
      "\u001b[34mDEBUG:__main__:  batch [   36/   45]\u001b[0m\n",
      "\u001b[34mbatch [   37/   45]\u001b[0m\n",
      "\u001b[34mDEBUG:__main__:  batch [   37/   45]\u001b[0m\n",
      "\u001b[34mbatch [   38/   45]\u001b[0m\n",
      "\u001b[34mDEBUG:__main__:  batch [   38/   45]\u001b[0m\n",
      "\u001b[34mbatch [   39/   45]\u001b[0m\n",
      "\u001b[34mDEBUG:__main__:  batch [   39/   45]\u001b[0m\n",
      "\u001b[34mbatch [   40/   45]\u001b[0m\n",
      "\u001b[34mDEBUG:__main__:  batch [   40/   45]\u001b[0m\n",
      "\u001b[34mbatch [   41/   45]\u001b[0m\n",
      "\u001b[34mDEBUG:__main__:  batch [   41/   45]\u001b[0m\n",
      "\u001b[34mbatch [   42/   45]\u001b[0m\n",
      "\u001b[34mDEBUG:__main__:  batch [   42/   45]\u001b[0m\n",
      "\u001b[34mbatch [   43/   45]\u001b[0m\n",
      "\u001b[34mDEBUG:__main__:  batch [   43/   45]\u001b[0m\n",
      "\u001b[34mbatch [   44/   45]\u001b[0m\n",
      "\u001b[34mDEBUG:__main__:  batch [   44/   45]\u001b[0m\n",
      "\u001b[34mbatch [   45/   45]\u001b[0m\n",
      "\u001b[34mDEBUG:__main__:  batch [   45/   45]\u001b[0m\n",
      "\u001b[34minference: end\u001b[0m\n",
      "\u001b[34mINFO:__main__:inference: end\u001b[0m\n",
      "\u001b[34m2022-10-10 15:47:18,341 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2022-10-10 15:47:18,341 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2022-10-10 15:47:18,342 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2022-10-10 15:47:30 Uploading - Uploading generated training model\n",
      "2022-10-10 15:47:51 Completed - Training job completed\n",
      "ProfilerReport-1665411844: NoIssuesFound\n",
      "Training seconds: 4943\n",
      "Billable seconds: 4943\n"
     ]
    }
   ],
   "source": [
    "estimator.fit({\"training\": inputs})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71964de-c07f-4bde-bafb-f634487a2bd5",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b979e79d-504f-4948-825f-f6fda26f2b2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sagemaker.pytorch.estimator.PyTorch at 0x12ac88e20>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4175ff-f795-4d08-afd3-6f62579d0ecd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
